{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-07T18:25:30.287562Z",
     "iopub.status.busy": "2024-03-07T18:25:30.287183Z",
     "iopub.status.idle": "2024-03-07T18:31:11.360528Z",
     "shell.execute_reply": "2024-03-07T18:31:11.359406Z",
     "shell.execute_reply.started": "2024-03-07T18:25:30.287531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/nbukhanchenko/pytorch-ts\n",
      "  Cloning https://github.com/nbukhanchenko/pytorch-ts to /tmp/pip-req-build-v_ulhyju\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nbukhanchenko/pytorch-ts /tmp/pip-req-build-v_ulhyju\n",
      "  Resolved https://github.com/nbukhanchenko/pytorch-ts to commit 0c2dc556629b6bd772570a3201389f2ec657482c\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorchts==0.7.0) (2.1.2)\n",
      "Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from pytorchts==0.7.0) (2.2.0.post0)\n",
      "Requirement already satisfied: protobuf~=3.20.3 in /opt/conda/lib/python3.10/site-packages (from pytorchts==0.7.0) (3.20.3)\n",
      "Collecting gluonts>=0.13.0 (from pytorchts==0.7.0)\n",
      "  Downloading gluonts-0.14.4-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: holidays in /opt/conda/lib/python3.10/site-packages (from pytorchts==0.7.0) (0.24)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from pytorchts==0.7.0) (3.7.5)\n",
      "Collecting diffusers (from pytorchts==0.7.0)\n",
      "  Downloading diffusers-0.26.3-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy~=1.16 in /opt/conda/lib/python3.10/site-packages (from gluonts>=0.13.0->pytorchts==0.7.0) (1.26.4)\n",
      "Requirement already satisfied: pandas<2.2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gluonts>=0.13.0->pytorchts==0.7.0) (2.1.4)\n",
      "Requirement already satisfied: pydantic<3,>=1.7 in /opt/conda/lib/python3.10/site-packages (from gluonts>=0.13.0->pytorchts==0.7.0) (2.5.3)\n",
      "Requirement already satisfied: tqdm~=4.23 in /opt/conda/lib/python3.10/site-packages (from gluonts>=0.13.0->pytorchts==0.7.0) (4.66.1)\n",
      "Requirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.10/site-packages (from gluonts>=0.13.0->pytorchts==0.7.0) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gluonts>=0.13.0->pytorchts==0.7.0) (4.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->pytorchts==0.7.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->pytorchts==0.7.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->pytorchts==0.7.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->pytorchts==0.7.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->pytorchts==0.7.0) (2024.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers->pytorchts==0.7.0) (6.11.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in /opt/conda/lib/python3.10/site-packages (from diffusers->pytorchts==0.7.0) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers->pytorchts==0.7.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers->pytorchts==0.7.0) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers->pytorchts==0.7.0) (0.4.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers->pytorchts==0.7.0) (9.5.0)\n",
      "Requirement already satisfied: hijri-converter in /opt/conda/lib/python3.10/site-packages (from holidays->pytorchts==0.7.0) (2.3.1)\n",
      "Requirement already satisfied: korean-lunar-calendar in /opt/conda/lib/python3.10/site-packages (from holidays->pytorchts==0.7.0) (0.3.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from holidays->pytorchts==0.7.0) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pytorchts==0.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pytorchts==0.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pytorchts==0.7.0) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pytorchts==0.7.0) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pytorchts==0.7.0) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pytorchts==0.7.0) (3.1.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->pytorchts==0.7.0) (6.0.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->pytorchts==0.7.0) (1.3.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning->pytorchts==0.7.0) (0.10.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->pytorchts==0.7.0) (3.9.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning->pytorchts==0.7.0) (69.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.0->gluonts>=0.13.0->pytorchts==0.7.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.0->gluonts>=0.13.0->pytorchts==0.7.0) (2023.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts>=0.13.0->pytorchts==0.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts>=0.13.0->pytorchts==0.7.0) (2.14.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil->holidays->pytorchts==0.7.0) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers->pytorchts==0.7.0) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->pytorchts==0.7.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers->pytorchts==0.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers->pytorchts==0.7.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers->pytorchts==0.7.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers->pytorchts==0.7.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->pytorchts==0.7.0) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->pytorchts==0.7.0) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->pytorchts==0.7.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->pytorchts==0.7.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->pytorchts==0.7.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->pytorchts==0.7.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->pytorchts==0.7.0) (4.0.3)\n",
      "Downloading gluonts-0.14.4-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diffusers-0.26.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pytorchts\n",
      "  Building wheel for pytorchts (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytorchts: filename=pytorchts-0.7.0-py3-none-any.whl size=124397 sha256=9271eaf60e875204cc79dc56a5bd72423757a6e92e85f02d471f07da6dddc03d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-h5njkwj8/wheels/72/4a/96/811c880612ee9a2607eef4d77cf69e8361df34c3c3ab80afdd\n",
      "Successfully built pytorchts\n",
      "Installing collected packages: gluonts, diffusers, pytorchts\n",
      "Successfully installed diffusers-0.26.3 gluonts-0.14.4 pytorchts-0.7.0\n",
      "Collecting gluonts==0.14.3\n",
      "  Downloading gluonts-0.14.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting numpy~=1.16 (from gluonts==0.14.3)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas<3,>=1.0 (from gluonts==0.14.3)\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting pydantic<3,>=1.7 (from gluonts==0.14.3)\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm~=4.23 (from gluonts==0.14.3)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toolz~=0.10 (from gluonts==0.14.3)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting typing-extensions~=4.0 (from gluonts==0.14.3)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas<3,>=1.0->gluonts==0.14.3)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas<3,>=1.0->gluonts==0.14.3)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3,>=1.0->gluonts==0.14.3)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.7->gluonts==0.14.3)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic<3,>=1.7->gluonts==0.14.3)\n",
      "  Downloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas<3,>=1.0->gluonts==0.14.3)\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading gluonts-0.14.3-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, typing-extensions, tqdm, toolz, six, numpy, annotated-types, python-dateutil, pydantic-core, pydantic, pandas, gluonts\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2023.4\n",
      "    Uninstalling tzdata-2023.4:\n",
      "      Successfully uninstalled tzdata-2023.4\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: toolz\n",
      "    Found existing installation: toolz 0.12.1\n",
      "    Uninstalling toolz-0.12.1:\n",
      "      Successfully uninstalled toolz-0.12.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.6.0\n",
      "    Uninstalling annotated-types-0.6.0:\n",
      "      Successfully uninstalled annotated-types-0.6.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "  Attempting uninstall: gluonts\n",
      "    Found existing installation: gluonts 0.14.4\n",
      "    Uninstalling gluonts-0.14.4:\n",
      "      Successfully uninstalled gluonts-0.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\n",
      "keras-nlp 0.8.1 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "jupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "rmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\n",
      "tensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "textblob 0.18.0.post0 requires nltk>=3.8, but you have nltk 3.2.4 which is incompatible.\n",
      "xarray 2024.2.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.6.0 gluonts-0.14.3 numpy-1.26.4 pandas-2.2.0 pydantic-2.6.1 pydantic-core-2.16.2 python-dateutil-2.9.0.post0 pytz-2024.1 six-1.16.0 toolz-0.12.1 tqdm-4.66.2 typing-extensions-4.10.0 tzdata-2024.1\n",
      "Collecting seaborn==0.13.1\n",
      "  Downloading seaborn-0.13.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting numpy!=1.24.0,>=1.20 (from seaborn==0.13.1)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pandas>=1.2 (from seaborn==0.13.1)\n",
      "  Using cached pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn==0.13.1)\n",
      "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting packaging>=20.0 (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting python-dateutil>=2.7 (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn==0.13.1)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn==0.13.1)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn==0.13.1)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading seaborn-0.13.1-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, pyparsing, pillow, packaging, numpy, kiwisolver, fonttools, cycler, python-dateutil, contourpy, pandas, matplotlib, seaborn\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.1\n",
      "    Uninstalling pytz-2024.1:\n",
      "      Successfully uninstalled pytz-2024.1\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2024.1\n",
      "    Uninstalling tzdata-2024.1:\n",
      "      Successfully uninstalled tzdata-2024.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.1\n",
      "    Uninstalling pyparsing-3.1.1:\n",
      "      Successfully uninstalled pyparsing-3.1.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.5.0\n",
      "    Uninstalling Pillow-9.5.0:\n",
      "      Successfully uninstalled Pillow-9.5.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.5\n",
      "    Uninstalling kiwisolver-1.4.5:\n",
      "      Successfully uninstalled kiwisolver-1.4.5\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.47.0\n",
      "    Uninstalling fonttools-4.47.0:\n",
      "      Successfully uninstalled fonttools-4.47.0\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.12.1\n",
      "    Uninstalling cycler-0.12.1:\n",
      "      Successfully uninstalled cycler-0.12.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.2.0\n",
      "    Uninstalling contourpy-1.2.0:\n",
      "      Successfully uninstalled contourpy-1.2.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.0\n",
      "    Uninstalling pandas-2.2.0:\n",
      "      Successfully uninstalled pandas-2.2.0\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.5\n",
      "    Uninstalling matplotlib-3.7.5:\n",
      "      Successfully uninstalled matplotlib-3.7.5\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.2\n",
      "    Uninstalling seaborn-0.12.2:\n",
      "      Successfully uninstalled seaborn-0.12.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\n",
      "keras-nlp 0.8.1 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.1 which is incompatible.\n",
      "distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n",
      "jupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "rmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\n",
      "tensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "textblob 0.18.0.post0 requires nltk>=3.8, but you have nltk 3.2.4 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed contourpy-1.2.0 cycler-0.12.1 fonttools-4.49.0 kiwisolver-1.4.5 matplotlib-3.8.3 numpy-1.26.4 packaging-23.2 pandas-2.2.1 pillow-10.2.0 pyparsing-3.1.2 python-dateutil-2.9.0.post0 pytz-2024.1 seaborn-0.13.1 six-1.16.0 tzdata-2024.1\n",
      "Collecting pandas==2.2.0\n",
      "  Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting numpy<2,>=1.22.4 (from pandas==2.2.0)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas==2.2.0)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas==2.2.0)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.0)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.0)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.1\n",
      "    Uninstalling pytz-2024.1:\n",
      "      Successfully uninstalled pytz-2024.1\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2024.1\n",
      "    Uninstalling tzdata-2024.1:\n",
      "      Successfully uninstalled tzdata-2024.1\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.1\n",
      "    Uninstalling pandas-2.2.1:\n",
      "      Successfully uninstalled pandas-2.2.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "keras-nlp 0.8.1 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n",
      "jupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "rmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\n",
      "tensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "textblob 0.18.0.post0 requires nltk>=3.8, but you have nltk 3.2.4 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.0 python-dateutil-2.9.0.post0 pytz-2024.1 six-1.16.0 tzdata-2024.1\n",
      "Collecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "keras-nlp 0.8.1 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "albumentations 1.4.0 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\n",
      "pylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "rmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\n",
      "tensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "woodwork 0.28.0 requires numpy<2.0.0,>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.23.5\n",
      "Collecting lightning==2.1.3\n",
      "  Downloading lightning-2.1.3-py3-none-any.whl.metadata (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML<8.0,>=5.4 (from lightning==2.1.3)\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting fsspec<2025.0,>=2022.5.0 (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning==2.1.3)\n",
      "  Downloading lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting numpy<3.0,>=1.17.2 (from lightning==2.1.3)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting packaging<25.0,>=20.0 (from lightning==2.1.3)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting torch<4.0,>=1.12.0 (from lightning==2.1.3)\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning==2.1.3)\n",
      "  Downloading torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tqdm<6.0,>=4.57.0 (from lightning==2.1.3)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<6.0,>=4.0.0 (from lightning==2.1.3)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pytorch-lightning (from lightning==2.1.3)\n",
      "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting setuptools (from lightning-utilities<2.0,>=0.8.0->lightning==2.1.3)\n",
      "  Downloading setuptools-69.1.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting filelock (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sympy (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch<4.0,>=1.12.0->lightning==2.1.3)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3)\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Downloading lightning-2.1.3-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-69.1.1-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, tqdm, sympy, setuptools, PyYAML, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, idna, fsspec, frozenlist, filelock, attrs, async-timeout, yarl, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lightning-utilities, jinja2, aiosignal, nvidia-cusolver-cu12, aiohttp, torch, torchmetrics, pytorch-lightning, lightning\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.0.3\n",
      "    Uninstalling setuptools-69.0.3:\n",
      "      Successfully uninstalled setuptools-69.0.3\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.2.1\n",
      "    Uninstalling networkx-3.2.1:\n",
      "      Successfully uninstalled networkx-3.2.1\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.0.4\n",
      "    Uninstalling multidict-6.0.4:\n",
      "      Successfully uninstalled multidict-6.0.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.1\n",
      "    Uninstalling frozenlist-1.4.1:\n",
      "      Successfully uninstalled frozenlist-1.4.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.2.0\n",
      "    Uninstalling attrs-23.2.0:\n",
      "      Successfully uninstalled attrs-23.2.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.3\n",
      "    Uninstalling async-timeout-4.0.3:\n",
      "      Successfully uninstalled async-timeout-4.0.3\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.9.3\n",
      "    Uninstalling yarl-1.9.3:\n",
      "      Successfully uninstalled yarl-1.9.3\n",
      "  Attempting uninstall: lightning-utilities\n",
      "    Found existing installation: lightning-utilities 0.10.1\n",
      "    Uninstalling lightning-utilities-0.10.1:\n",
      "      Successfully uninstalled lightning-utilities-0.10.1\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.1\n",
      "    Uninstalling aiosignal-1.3.1:\n",
      "      Successfully uninstalled aiosignal-1.3.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.1\n",
      "    Uninstalling aiohttp-3.9.1:\n",
      "      Successfully uninstalled aiohttp-3.9.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.2\n",
      "    Uninstalling torch-2.1.2:\n",
      "      Successfully uninstalled torch-2.1.2\n",
      "  Attempting uninstall: torchmetrics\n",
      "    Found existing installation: torchmetrics 1.3.1\n",
      "    Uninstalling torchmetrics-1.3.1:\n",
      "      Successfully uninstalled torchmetrics-1.3.1\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 2.2.0.post0\n",
      "    Uninstalling pytorch-lightning-2.2.0.post0:\n",
      "      Successfully uninstalled pytorch-lightning-2.2.0.post0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\n",
      "keras-nlp 0.8.1 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "gcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2024.2.0 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n",
      "jupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "rmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\n",
      "tensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "virtualenv 20.21.0 requires platformdirs<4,>=2.4, but you have platformdirs 4.2.0 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 PyYAML-6.0.1 aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 filelock-3.13.1 frozenlist-1.4.1 fsspec-2024.2.0 idna-3.6 jinja2-3.1.3 lightning-2.1.3 lightning-utilities-0.10.1 mpmath-1.3.0 multidict-6.0.4 networkx-3.2.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 packaging-23.2 pytorch-lightning-2.2.1 setuptools-69.1.1 sympy-1.12 torch-2.2.1 torchmetrics-1.3.1 tqdm-4.66.2 triton-2.2.0 typing-extensions-4.10.0 yarl-1.9.4\n",
      "Collecting diffusers==0.25.1\n",
      "  Downloading diffusers-0.25.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting importlib-metadata (from diffusers==0.25.1)\n",
      "  Downloading importlib_metadata-7.0.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting filelock (from diffusers==0.25.1)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub>=0.20.2 (from diffusers==0.25.1)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy (from diffusers==0.25.1)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting regex!=2019.12.17 (from diffusers==0.25.1)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from diffusers==0.25.1)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.3.1 (from diffusers==0.25.1)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting Pillow (from diffusers==0.25.1)\n",
      "  Using cached pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.2->diffusers==0.25.1)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.20.2->diffusers==0.25.1)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.2->diffusers==0.25.1)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.20.2->diffusers==0.25.1)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging>=20.9 (from huggingface-hub>=0.20.2->diffusers==0.25.1)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata->diffusers==0.25.1)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->diffusers==0.25.1)\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->diffusers==0.25.1)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->diffusers==0.25.1)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->diffusers==0.25.1)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading diffusers-0.25.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading importlib_metadata-7.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for multidict: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/multidict-6.0.4.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: zipp, urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, Pillow, packaging, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, importlib-metadata, huggingface-hub, diffusers\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.17.0\n",
      "    Uninstalling zipp-3.17.0:\n",
      "      Successfully uninstalled zipp-3.17.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.2\n",
      "    Uninstalling safetensors-0.4.2:\n",
      "      Successfully uninstalled safetensors-0.4.2\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.12.25\n",
      "    Uninstalling regex-2023.12.25:\n",
      "      Successfully uninstalled regex-2023.12.25\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.11.0\n",
      "    Uninstalling importlib-metadata-6.11.0:\n",
      "      Successfully uninstalled importlib-metadata-6.11.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.26.3\n",
      "    Uninstalling diffusers-0.26.3:\n",
      "      Successfully uninstalled diffusers-0.26.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "datasets 2.1.0 requires aiohttp, which is not installed.\n",
      "fury 0.9.0 requires aiohttp>=3.8.4, which is not installed.\n",
      "gcsfs 2023.12.2.post1 requires aiohttp!=4.0.0a0,!=4.0.0a1, which is not installed.\n",
      "jupyter-server-proxy 4.1.0 requires aiohttp, which is not installed.\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\n",
      "keras-nlp 0.8.1 requires keras-core, which is not installed.\n",
      "s3fs 2024.2.0 requires aiohttp!=4.0.0a0,!=4.0.0a1, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "yarl 1.9.4 requires multidict>=4.0, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\n",
      "botocore 1.34.34 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.0 which is incompatible.\n",
      "distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "gcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2024.2.0 which is incompatible.\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n",
      "google-cloud-pubsub 2.19.0 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\n",
      "jupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "opentelemetry-api 1.22.0 requires importlib-metadata<7.0,>=6.0, but you have importlib-metadata 7.0.2 which is incompatible.\n",
      "osmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\n",
      "rmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\n",
      "tensorstore 0.1.53 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "virtualenv 20.21.0 requires platformdirs<4,>=2.4, but you have platformdirs 4.2.0 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Pillow-10.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 diffusers-0.25.1 filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.21.4 idna-3.6 importlib-metadata-7.0.1 numpy-1.26.4 packaging-23.2 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 tqdm-4.66.2 typing-extensions-4.10.0 urllib3-2.1.0 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"git+https://github.com/nbukhanchenko/trans-timegrad\"\n",
    "# !pip install --upgrade --force-reinstall gluonts==0.14.3\n",
    "# !pip install --upgrade --force-reinstall seaborn==0.13.1\n",
    "# !pip install --upgrade --force-reinstall pandas==2.2.0\n",
    "# !pip install --upgrade --force-reinstall numpy==1.23.5\n",
    "# !pip install --upgrade --force-reinstall lightning==2.1.3\n",
    "# !pip install --upgrade --force-reinstall diffusers==0.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "from diffusers import (\n",
    "    DDPMScheduler,\n",
    "    PNDMScheduler,\n",
    "    DDIMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    KDPM2DiscreteScheduler,\n",
    "    DEISMultistepScheduler,\n",
    ")\n",
    "\n",
    "from pts.model.time_grad import TimeGradEstimator\n",
    "from pts.dataset.repository.datasets import dataset_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_for_percentile(p):\n",
    "    return (p / 100.0) ** 0.3\n",
    "\n",
    "def plot(\n",
    "    target,\n",
    "    forecast,\n",
    "    prediction_length,\n",
    "    prediction_intervals=(50.0, 90.0),\n",
    "    color=\"g\",\n",
    "    fname=None,\n",
    "):\n",
    "    label_prefix = \"\"\n",
    "    rows = 4\n",
    "    cols = 4\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(24, 24))\n",
    "    axx = axs.ravel()\n",
    "    seq_len, target_dim = target.shape\n",
    "\n",
    "    ps = [50.0] + [\n",
    "        50.0 + f * c / 2.0 for c in prediction_intervals for f in [-1.0, +1.0]\n",
    "    ]\n",
    "\n",
    "    percentiles_sorted = sorted(set(ps))\n",
    "\n",
    "    for dim in range(0, min(rows * cols, target_dim)):\n",
    "        ax = axx[dim]\n",
    "\n",
    "        target[-2 * prediction_length :][dim].plot(ax=ax)\n",
    "\n",
    "        ps_data = [forecast.quantile(p / 100.0)[:, dim] for p in percentiles_sorted]\n",
    "        i_p50 = len(percentiles_sorted) // 2\n",
    "\n",
    "        p50_data = ps_data[i_p50]\n",
    "        p50_series = pd.Series(data=p50_data, index=forecast.index)\n",
    "        p50_series.plot(color=color, ls=\"-\", label=f\"{label_prefix}median\", ax=ax)\n",
    "\n",
    "        for i in range(len(percentiles_sorted) // 2):\n",
    "            ptile = percentiles_sorted[i]\n",
    "            alpha = alpha_for_percentile(ptile)\n",
    "            ax.fill_between(\n",
    "                forecast.index,\n",
    "                ps_data[i],\n",
    "                ps_data[-i - 1],\n",
    "                facecolor=color,\n",
    "                alpha=alpha,\n",
    "                interpolate=True,\n",
    "            )\n",
    "            # Hack to create labels for the error intervals.\n",
    "            # Doesn't actually plot anything, because we only pass a single data point\n",
    "            pd.Series(data=p50_data[:1], index=forecast.index[:1]).plot(\n",
    "                color=color,\n",
    "                alpha=alpha,\n",
    "                linewidth=10,\n",
    "                label=f\"{label_prefix}{100 - ptile * 2}%\",\n",
    "                ax=ax,\n",
    "            )\n",
    "\n",
    "    legend = [\"observations\", \"median prediction\"] + [\n",
    "        f\"{k}% prediction interval\" for k in prediction_intervals\n",
    "    ][::-1]\n",
    "    axx[0].legend(legend, loc=\"upper left\")\n",
    "\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "\n",
    "def prepare_dataset(dataset_name):\n",
    "    dataset = get_dataset(dataset_name, regenerate=False)\n",
    "\n",
    "    train_grouper = MultivariateGrouper(\n",
    "        max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality)\n",
    "    )\n",
    "    test_grouper = MultivariateGrouper(\n",
    "        num_test_dates=int(len(dataset.test) / len(dataset.train)),\n",
    "        max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train\": train_grouper(dataset.train),\n",
    "        \"test\": test_grouper(dataset.test),\n",
    "        \"metadata\": dataset.metadata\n",
    "    }\n",
    "\n",
    "def prepare_predictor(dataset, max_epochs=256,\n",
    "                      num_train_timesteps=150, beta_start=1e-4, beta_end=0.1, beta_schedule=\"linear\",\n",
    "                      context_length_coef=3, num_layers=2, hidden_size=64, lr=3e-4, weight_decay=1e-8, dropout_rate=0.1,\n",
    "                      lags_seq=[1], num_inference_steps=149, batch_size=64, num_batches_per_epoch=64):\n",
    "#     scheduler = PNDMScheduler(\n",
    "#         num_train_timesteps=num_train_timesteps,\n",
    "#         beta_start=beta_start,\n",
    "#         beta_end=beta_end,\n",
    "#         beta_schedule=beta_schedule,\n",
    "#     )\n",
    "\n",
    "#     scheduler = DDPMScheduler(\n",
    "#         num_train_timesteps=num_train_timesteps,\n",
    "#         beta_start=beta_start,\n",
    "#         beta_end=beta_end,\n",
    "#         beta_schedule=beta_schedule,\n",
    "#     )\n",
    "\n",
    "    scheduler = DEISMultistepScheduler(\n",
    "        num_train_timesteps=num_train_timesteps,\n",
    "        beta_start=beta_start,\n",
    "        beta_end=beta_end,\n",
    "        beta_schedule=beta_schedule,\n",
    "    )\n",
    "\n",
    "    estimator = TimeGradEstimator(\n",
    "        freq=dataset[\"metadata\"].freq,\n",
    "        prediction_length=dataset[\"metadata\"].prediction_length,\n",
    "        input_size=int(dataset[\"metadata\"].feat_static_cat[0].cardinality),\n",
    "        scheduler=scheduler,\n",
    "        context_length=dataset[\"metadata\"].prediction_length * context_length_coef,\n",
    "        num_layers=num_layers,\n",
    "        hidden_size=hidden_size,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        dropout_rate=dropout_rate,\n",
    "        scaling=\"mean\",\n",
    "        lags_seq=lags_seq,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        batch_size=batch_size,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "        trainer_kwargs=dict(max_epochs=max_epochs, accelerator=\"gpu\", devices=\"1\"),\n",
    "    )\n",
    "\n",
    "    return estimator.train(dataset[\"train\"], cache_data=True, shuffle_buffer_length=1024)\n",
    "\n",
    "def prepare_metrics(dataset, predictor, num_samples=100):\n",
    "    evaluator = MultivariateEvaluator(\n",
    "        quantiles=(np.arange(20) / 20.0)[1:], target_agg_funcs={\"sum\": np.sum}\n",
    "    )\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset[\"test\"], predictor=predictor, num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    targets = list(ts_it)\n",
    "    agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset[\"test\"]))\n",
    "\n",
    "    return forecasts, targets, agg_metric\n",
    "\n",
    "def prepare_statistics(dataset, forecasts, targets, agg_metric, precision=3):\n",
    "    print(\"CRPS: {}\".format(round(agg_metric[\"mean_wQuantileLoss\"], precision)))\n",
    "    print(\"ND: {}\".format(round(agg_metric[\"ND\"], precision)))\n",
    "    print(\"NRMSE: {}\".format(round(agg_metric[\"NRMSE\"], precision)))\n",
    "    print(\"MSE: {}\".format(round(agg_metric[\"MSE\"], precision)))\n",
    "\n",
    "    print(\"-\" * 32)\n",
    "\n",
    "    print(\"CRPS-Sum: {}\".format(round(agg_metric[\"m_sum_mean_wQuantileLoss\"], precision)))\n",
    "    print(\"ND-Sum: {}\".format(round(agg_metric[\"m_sum_ND\"], precision)))\n",
    "    print(\"NRMSE-Sum: {}\".format(round(agg_metric[\"m_sum_NRMSE\"], precision)))\n",
    "    print(\"MSE-Sum: {}\".format(round(agg_metric[\"m_sum_MSE\"], precision)))\n",
    "\n",
    "    plot(\n",
    "        target=targets[0],\n",
    "        forecast=forecasts[0],\n",
    "        prediction_length=dataset[\"metadata\"].prediction_length,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use different types of schedulers\n",
    "# check different trainer_kwargs\n",
    "HYPERPARAMETERS = {\n",
    "    \"max_epochs\":            [128, 256, 512],\n",
    "    \"num_train_timesteps\":   [50, 100, 150, 200, 250],       # explore\n",
    "    \"beta_start\":            [1e-4],\n",
    "    \"beta_end\":              [0.1],\n",
    "    \"beta_schedule\":         [\"linear\"],                     # explore\n",
    "    \"context_length_coef\":   [1, 2, 3, 4, 5],                # explore\n",
    "    \"num_layers\":            [2, 3, 5],                      # explore\n",
    "    \"hidden_size\":           [32, 64, 128],                  # explore\n",
    "    \"lr\":                    [1e-5, 5e-5, 1e-4, 5e-4, 1e-3], # explore\n",
    "    \"weight_decay\":          [1e-9, 1e-8, 1e-7],\n",
    "    \"dropout_rate\":          [0.0, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"lags_seq\":              [None, [1]],                    # explore\n",
    "    \"num_inference_steps\":   [49, 99, 149, 199, 249],\n",
    "    \"batch_size\":            [32, 64, 128],\n",
    "    \"num_batches_per_epoch\": [32, 64, 128],\n",
    "    \"num_samples\":           [50, 100, 200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/time_feature/_base.py:243: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = to_offset(freq_str)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                             | Out sizes        \n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 186 K  | [[1, 1], [1, 1], [1, 72, 5], [1, 72, 137], [1, 72, 137], [1, 24, 5]] | [1, 100, 24, 137]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "186 K     Trainable params\n",
      "0         Non-trainable params\n",
      "186 K     Total params\n",
      "0.745     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c0d2006c4f426f96143559b9105e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41120 (best 0.41120), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.31175 (best 0.31175), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.14226 (best 0.14226), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.08811 (best 0.08811), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.07767 (best 0.07767), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.07093 (best 0.07093), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.06598 (best 0.06598), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.06143 (best 0.06143), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.05992 (best 0.05992), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.05841 (best 0.05841), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.05691 (best 0.05691), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.05547 (best 0.05547), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.05533 (best 0.05533), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.05484 (best 0.05484), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' reached 0.05418 (best 0.05418), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=14-step=960.ckpt' as top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.05330 (best 0.05330), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' reached 0.05304 (best 0.05304), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=16-step=1088.ckpt' as top 1\n",
      "Epoch 17, global step 1152: 'train_loss' reached 0.05207 (best 0.05207), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=17-step=1152.ckpt' as top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.05185 (best 0.05185), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' reached 0.05132 (best 0.05132), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=19-step=1280.ckpt' as top 1\n",
      "Epoch 20, global step 1344: 'train_loss' reached 0.05077 (best 0.05077), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=20-step=1344.ckpt' as top 1\n",
      "Epoch 21, global step 1408: 'train_loss' reached 0.05069 (best 0.05069), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=21-step=1408.ckpt' as top 1\n",
      "Epoch 22, global step 1472: 'train_loss' reached 0.05032 (best 0.05032), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=22-step=1472.ckpt' as top 1\n",
      "Epoch 23, global step 1536: 'train_loss' reached 0.04983 (best 0.04983), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=23-step=1536.ckpt' as top 1\n",
      "Epoch 24, global step 1600: 'train_loss' reached 0.04952 (best 0.04952), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=24-step=1600.ckpt' as top 1\n",
      "Epoch 25, global step 1664: 'train_loss' reached 0.04904 (best 0.04904), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=25-step=1664.ckpt' as top 1\n",
      "Epoch 26, global step 1728: 'train_loss' reached 0.04796 (best 0.04796), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=26-step=1728.ckpt' as top 1\n",
      "Epoch 27, global step 1792: 'train_loss' was not in top 1\n",
      "Epoch 28, global step 1856: 'train_loss' was not in top 1\n",
      "Epoch 29, global step 1920: 'train_loss' was not in top 1\n",
      "Epoch 30, global step 1984: 'train_loss' reached 0.04692 (best 0.04692), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=30-step=1984.ckpt' as top 1\n",
      "Epoch 31, global step 2048: 'train_loss' was not in top 1\n",
      "Epoch 32, global step 2112: 'train_loss' was not in top 1\n",
      "Epoch 33, global step 2176: 'train_loss' was not in top 1\n",
      "Epoch 34, global step 2240: 'train_loss' reached 0.04639 (best 0.04639), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=34-step=2240.ckpt' as top 1\n",
      "Epoch 35, global step 2304: 'train_loss' was not in top 1\n",
      "Epoch 36, global step 2368: 'train_loss' reached 0.04616 (best 0.04616), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=36-step=2368.ckpt' as top 1\n",
      "Epoch 37, global step 2432: 'train_loss' reached 0.04609 (best 0.04609), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=37-step=2432.ckpt' as top 1\n",
      "Epoch 38, global step 2496: 'train_loss' reached 0.04511 (best 0.04511), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=38-step=2496.ckpt' as top 1\n",
      "Epoch 39, global step 2560: 'train_loss' was not in top 1\n",
      "Epoch 40, global step 2624: 'train_loss' was not in top 1\n",
      "Epoch 41, global step 2688: 'train_loss' reached 0.04499 (best 0.04499), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=41-step=2688.ckpt' as top 1\n",
      "Epoch 42, global step 2752: 'train_loss' was not in top 1\n",
      "Epoch 43, global step 2816: 'train_loss' was not in top 1\n",
      "Epoch 44, global step 2880: 'train_loss' was not in top 1\n",
      "Epoch 45, global step 2944: 'train_loss' reached 0.04478 (best 0.04478), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=45-step=2944.ckpt' as top 1\n",
      "Epoch 46, global step 3008: 'train_loss' reached 0.04426 (best 0.04426), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=46-step=3008.ckpt' as top 1\n",
      "Epoch 47, global step 3072: 'train_loss' was not in top 1\n",
      "Epoch 48, global step 3136: 'train_loss' reached 0.04417 (best 0.04417), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=48-step=3136.ckpt' as top 1\n",
      "Epoch 49, global step 3200: 'train_loss' reached 0.04405 (best 0.04405), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=49-step=3200.ckpt' as top 1\n",
      "Epoch 50, global step 3264: 'train_loss' reached 0.04387 (best 0.04387), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=50-step=3264.ckpt' as top 1\n",
      "Epoch 51, global step 3328: 'train_loss' reached 0.04353 (best 0.04353), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=51-step=3328.ckpt' as top 1\n",
      "Epoch 52, global step 3392: 'train_loss' was not in top 1\n",
      "Epoch 53, global step 3456: 'train_loss' was not in top 1\n",
      "Epoch 54, global step 3520: 'train_loss' reached 0.04304 (best 0.04304), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=54-step=3520.ckpt' as top 1\n",
      "Epoch 55, global step 3584: 'train_loss' was not in top 1\n",
      "Epoch 56, global step 3648: 'train_loss' was not in top 1\n",
      "Epoch 57, global step 3712: 'train_loss' reached 0.04274 (best 0.04274), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=57-step=3712.ckpt' as top 1\n",
      "Epoch 58, global step 3776: 'train_loss' was not in top 1\n",
      "Epoch 59, global step 3840: 'train_loss' was not in top 1\n",
      "Epoch 60, global step 3904: 'train_loss' reached 0.04240 (best 0.04240), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=60-step=3904.ckpt' as top 1\n",
      "Epoch 61, global step 3968: 'train_loss' was not in top 1\n",
      "Epoch 62, global step 4032: 'train_loss' was not in top 1\n",
      "Epoch 63, global step 4096: 'train_loss' was not in top 1\n",
      "Epoch 64, global step 4160: 'train_loss' was not in top 1\n",
      "Epoch 65, global step 4224: 'train_loss' reached 0.04213 (best 0.04213), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=65-step=4224.ckpt' as top 1\n",
      "Epoch 66, global step 4288: 'train_loss' was not in top 1\n",
      "Epoch 67, global step 4352: 'train_loss' reached 0.04212 (best 0.04212), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=67-step=4352.ckpt' as top 1\n",
      "Epoch 68, global step 4416: 'train_loss' was not in top 1\n",
      "Epoch 69, global step 4480: 'train_loss' was not in top 1\n",
      "Epoch 70, global step 4544: 'train_loss' was not in top 1\n",
      "Epoch 71, global step 4608: 'train_loss' was not in top 1\n",
      "Epoch 72, global step 4672: 'train_loss' was not in top 1\n",
      "Epoch 73, global step 4736: 'train_loss' reached 0.04143 (best 0.04143), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=73-step=4736.ckpt' as top 1\n",
      "Epoch 74, global step 4800: 'train_loss' was not in top 1\n",
      "Epoch 75, global step 4864: 'train_loss' was not in top 1\n",
      "Epoch 76, global step 4928: 'train_loss' was not in top 1\n",
      "Epoch 77, global step 4992: 'train_loss' was not in top 1\n",
      "Epoch 78, global step 5056: 'train_loss' was not in top 1\n",
      "Epoch 79, global step 5120: 'train_loss' was not in top 1\n",
      "Epoch 80, global step 5184: 'train_loss' reached 0.04105 (best 0.04105), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=80-step=5184.ckpt' as top 1\n",
      "Epoch 81, global step 5248: 'train_loss' was not in top 1\n",
      "Epoch 82, global step 5312: 'train_loss' was not in top 1\n",
      "Epoch 83, global step 5376: 'train_loss' reached 0.04102 (best 0.04102), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=83-step=5376.ckpt' as top 1\n",
      "Epoch 84, global step 5440: 'train_loss' was not in top 1\n",
      "Epoch 85, global step 5504: 'train_loss' was not in top 1\n",
      "Epoch 86, global step 5568: 'train_loss' was not in top 1\n",
      "Epoch 87, global step 5632: 'train_loss' was not in top 1\n",
      "Epoch 88, global step 5696: 'train_loss' was not in top 1\n",
      "Epoch 89, global step 5760: 'train_loss' was not in top 1\n",
      "Epoch 90, global step 5824: 'train_loss' was not in top 1\n",
      "Epoch 91, global step 5888: 'train_loss' was not in top 1\n",
      "Epoch 92, global step 5952: 'train_loss' reached 0.04096 (best 0.04096), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=92-step=5952.ckpt' as top 1\n",
      "Epoch 93, global step 6016: 'train_loss' was not in top 1\n",
      "Epoch 94, global step 6080: 'train_loss' reached 0.04093 (best 0.04093), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=94-step=6080.ckpt' as top 1\n",
      "Epoch 95, global step 6144: 'train_loss' was not in top 1\n",
      "Epoch 96, global step 6208: 'train_loss' reached 0.04055 (best 0.04055), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=96-step=6208.ckpt' as top 1\n",
      "Epoch 97, global step 6272: 'train_loss' was not in top 1\n",
      "Epoch 98, global step 6336: 'train_loss' was not in top 1\n",
      "Epoch 99, global step 6400: 'train_loss' was not in top 1\n",
      "Epoch 100, global step 6464: 'train_loss' reached 0.04048 (best 0.04048), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=100-step=6464.ckpt' as top 1\n",
      "Epoch 101, global step 6528: 'train_loss' reached 0.03971 (best 0.03971), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=101-step=6528.ckpt' as top 1\n",
      "Epoch 102, global step 6592: 'train_loss' was not in top 1\n",
      "Epoch 103, global step 6656: 'train_loss' was not in top 1\n",
      "Epoch 104, global step 6720: 'train_loss' was not in top 1\n",
      "Epoch 105, global step 6784: 'train_loss' was not in top 1\n",
      "Epoch 106, global step 6848: 'train_loss' was not in top 1\n",
      "Epoch 107, global step 6912: 'train_loss' was not in top 1\n",
      "Epoch 108, global step 6976: 'train_loss' was not in top 1\n",
      "Epoch 109, global step 7040: 'train_loss' was not in top 1\n",
      "Epoch 110, global step 7104: 'train_loss' was not in top 1\n",
      "Epoch 111, global step 7168: 'train_loss' was not in top 1\n",
      "Epoch 112, global step 7232: 'train_loss' was not in top 1\n",
      "Epoch 113, global step 7296: 'train_loss' was not in top 1\n",
      "Epoch 114, global step 7360: 'train_loss' reached 0.03966 (best 0.03966), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=114-step=7360.ckpt' as top 1\n",
      "Epoch 115, global step 7424: 'train_loss' was not in top 1\n",
      "Epoch 116, global step 7488: 'train_loss' was not in top 1\n",
      "Epoch 117, global step 7552: 'train_loss' was not in top 1\n",
      "Epoch 118, global step 7616: 'train_loss' was not in top 1\n",
      "Epoch 119, global step 7680: 'train_loss' was not in top 1\n",
      "Epoch 120, global step 7744: 'train_loss' was not in top 1\n",
      "Epoch 121, global step 7808: 'train_loss' was not in top 1\n",
      "Epoch 122, global step 7872: 'train_loss' was not in top 1\n",
      "Epoch 123, global step 7936: 'train_loss' was not in top 1\n",
      "Epoch 124, global step 8000: 'train_loss' was not in top 1\n",
      "Epoch 125, global step 8064: 'train_loss' was not in top 1\n",
      "Epoch 126, global step 8128: 'train_loss' reached 0.03900 (best 0.03900), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=126-step=8128.ckpt' as top 1\n",
      "Epoch 127, global step 8192: 'train_loss' was not in top 1\n",
      "Epoch 128, global step 8256: 'train_loss' was not in top 1\n",
      "Epoch 129, global step 8320: 'train_loss' was not in top 1\n",
      "Epoch 130, global step 8384: 'train_loss' was not in top 1\n",
      "Epoch 131, global step 8448: 'train_loss' was not in top 1\n",
      "Epoch 132, global step 8512: 'train_loss' was not in top 1\n",
      "Epoch 133, global step 8576: 'train_loss' was not in top 1\n",
      "Epoch 134, global step 8640: 'train_loss' was not in top 1\n",
      "Epoch 135, global step 8704: 'train_loss' was not in top 1\n",
      "Epoch 136, global step 8768: 'train_loss' was not in top 1\n",
      "Epoch 137, global step 8832: 'train_loss' was not in top 1\n",
      "Epoch 138, global step 8896: 'train_loss' was not in top 1\n",
      "Epoch 139, global step 8960: 'train_loss' was not in top 1\n",
      "Epoch 140, global step 9024: 'train_loss' was not in top 1\n",
      "Epoch 141, global step 9088: 'train_loss' was not in top 1\n",
      "Epoch 142, global step 9152: 'train_loss' was not in top 1\n",
      "Epoch 143, global step 9216: 'train_loss' was not in top 1\n",
      "Epoch 144, global step 9280: 'train_loss' was not in top 1\n",
      "Epoch 145, global step 9344: 'train_loss' was not in top 1\n",
      "Epoch 146, global step 9408: 'train_loss' was not in top 1\n",
      "Epoch 147, global step 9472: 'train_loss' was not in top 1\n",
      "Epoch 148, global step 9536: 'train_loss' was not in top 1\n",
      "Epoch 149, global step 9600: 'train_loss' was not in top 1\n",
      "Epoch 150, global step 9664: 'train_loss' was not in top 1\n",
      "Epoch 151, global step 9728: 'train_loss' was not in top 1\n",
      "Epoch 152, global step 9792: 'train_loss' was not in top 1\n",
      "Epoch 153, global step 9856: 'train_loss' was not in top 1\n",
      "Epoch 154, global step 9920: 'train_loss' was not in top 1\n",
      "Epoch 155, global step 9984: 'train_loss' was not in top 1\n",
      "Epoch 156, global step 10048: 'train_loss' was not in top 1\n",
      "Epoch 157, global step 10112: 'train_loss' was not in top 1\n",
      "Epoch 158, global step 10176: 'train_loss' was not in top 1\n",
      "Epoch 159, global step 10240: 'train_loss' was not in top 1\n",
      "Epoch 160, global step 10304: 'train_loss' was not in top 1\n",
      "Epoch 161, global step 10368: 'train_loss' was not in top 1\n",
      "Epoch 162, global step 10432: 'train_loss' was not in top 1\n",
      "Epoch 163, global step 10496: 'train_loss' was not in top 1\n",
      "Epoch 164, global step 10560: 'train_loss' was not in top 1\n",
      "Epoch 165, global step 10624: 'train_loss' was not in top 1\n",
      "Epoch 166, global step 10688: 'train_loss' was not in top 1\n",
      "Epoch 167, global step 10752: 'train_loss' was not in top 1\n",
      "Epoch 168, global step 10816: 'train_loss' was not in top 1\n",
      "Epoch 169, global step 10880: 'train_loss' was not in top 1\n",
      "Epoch 170, global step 10944: 'train_loss' was not in top 1\n",
      "Epoch 171, global step 11008: 'train_loss' was not in top 1\n",
      "Epoch 172, global step 11072: 'train_loss' was not in top 1\n",
      "Epoch 173, global step 11136: 'train_loss' was not in top 1\n",
      "Epoch 174, global step 11200: 'train_loss' was not in top 1\n",
      "Epoch 175, global step 11264: 'train_loss' was not in top 1\n",
      "Epoch 176, global step 11328: 'train_loss' was not in top 1\n",
      "Epoch 177, global step 11392: 'train_loss' was not in top 1\n",
      "Epoch 178, global step 11456: 'train_loss' was not in top 1\n",
      "Epoch 179, global step 11520: 'train_loss' was not in top 1\n",
      "Epoch 180, global step 11584: 'train_loss' was not in top 1\n",
      "Epoch 181, global step 11648: 'train_loss' was not in top 1\n",
      "Epoch 182, global step 11712: 'train_loss' was not in top 1\n",
      "Epoch 183, global step 11776: 'train_loss' was not in top 1\n",
      "Epoch 184, global step 11840: 'train_loss' was not in top 1\n",
      "Epoch 185, global step 11904: 'train_loss' was not in top 1\n",
      "Epoch 186, global step 11968: 'train_loss' was not in top 1\n",
      "Epoch 187, global step 12032: 'train_loss' was not in top 1\n",
      "Epoch 188, global step 12096: 'train_loss' was not in top 1\n",
      "Epoch 189, global step 12160: 'train_loss' was not in top 1\n",
      "Epoch 190, global step 12224: 'train_loss' was not in top 1\n",
      "Epoch 191, global step 12288: 'train_loss' was not in top 1\n",
      "Epoch 192, global step 12352: 'train_loss' was not in top 1\n",
      "Epoch 193, global step 12416: 'train_loss' was not in top 1\n",
      "Epoch 194, global step 12480: 'train_loss' was not in top 1\n",
      "Epoch 195, global step 12544: 'train_loss' was not in top 1\n",
      "Epoch 196, global step 12608: 'train_loss' was not in top 1\n",
      "Epoch 197, global step 12672: 'train_loss' was not in top 1\n",
      "Epoch 198, global step 12736: 'train_loss' was not in top 1\n",
      "Epoch 199, global step 12800: 'train_loss' was not in top 1\n",
      "Epoch 200, global step 12864: 'train_loss' was not in top 1\n",
      "Epoch 201, global step 12928: 'train_loss' was not in top 1\n",
      "Epoch 202, global step 12992: 'train_loss' was not in top 1\n",
      "Epoch 203, global step 13056: 'train_loss' was not in top 1\n",
      "Epoch 204, global step 13120: 'train_loss' was not in top 1\n",
      "Epoch 205, global step 13184: 'train_loss' was not in top 1\n",
      "Epoch 206, global step 13248: 'train_loss' was not in top 1\n",
      "Epoch 207, global step 13312: 'train_loss' was not in top 1\n",
      "Epoch 208, global step 13376: 'train_loss' was not in top 1\n",
      "Epoch 209, global step 13440: 'train_loss' reached 0.03887 (best 0.03887), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=209-step=13440.ckpt' as top 1\n",
      "Epoch 210, global step 13504: 'train_loss' was not in top 1\n",
      "Epoch 211, global step 13568: 'train_loss' was not in top 1\n",
      "Epoch 212, global step 13632: 'train_loss' was not in top 1\n",
      "Epoch 213, global step 13696: 'train_loss' was not in top 1\n",
      "Epoch 214, global step 13760: 'train_loss' was not in top 1\n",
      "Epoch 215, global step 13824: 'train_loss' was not in top 1\n",
      "Epoch 216, global step 13888: 'train_loss' was not in top 1\n",
      "Epoch 217, global step 13952: 'train_loss' was not in top 1\n",
      "Epoch 218, global step 14016: 'train_loss' was not in top 1\n",
      "Epoch 219, global step 14080: 'train_loss' was not in top 1\n",
      "Epoch 220, global step 14144: 'train_loss' was not in top 1\n",
      "Epoch 221, global step 14208: 'train_loss' was not in top 1\n",
      "Epoch 222, global step 14272: 'train_loss' was not in top 1\n",
      "Epoch 223, global step 14336: 'train_loss' was not in top 1\n",
      "Epoch 224, global step 14400: 'train_loss' was not in top 1\n",
      "Epoch 225, global step 14464: 'train_loss' was not in top 1\n",
      "Epoch 226, global step 14528: 'train_loss' was not in top 1\n",
      "Epoch 227, global step 14592: 'train_loss' was not in top 1\n",
      "Epoch 228, global step 14656: 'train_loss' was not in top 1\n",
      "Epoch 229, global step 14720: 'train_loss' was not in top 1\n",
      "Epoch 230, global step 14784: 'train_loss' was not in top 1\n",
      "Epoch 231, global step 14848: 'train_loss' was not in top 1\n",
      "Epoch 232, global step 14912: 'train_loss' was not in top 1\n",
      "Epoch 233, global step 14976: 'train_loss' was not in top 1\n",
      "Epoch 234, global step 15040: 'train_loss' was not in top 1\n",
      "Epoch 235, global step 15104: 'train_loss' was not in top 1\n",
      "Epoch 236, global step 15168: 'train_loss' was not in top 1\n",
      "Epoch 237, global step 15232: 'train_loss' was not in top 1\n",
      "Epoch 238, global step 15296: 'train_loss' was not in top 1\n",
      "Epoch 239, global step 15360: 'train_loss' was not in top 1\n",
      "Epoch 240, global step 15424: 'train_loss' was not in top 1\n",
      "Epoch 241, global step 15488: 'train_loss' was not in top 1\n",
      "Epoch 242, global step 15552: 'train_loss' was not in top 1\n",
      "Epoch 243, global step 15616: 'train_loss' was not in top 1\n",
      "Epoch 244, global step 15680: 'train_loss' was not in top 1\n",
      "Epoch 245, global step 15744: 'train_loss' was not in top 1\n",
      "Epoch 246, global step 15808: 'train_loss' was not in top 1\n",
      "Epoch 247, global step 15872: 'train_loss' was not in top 1\n",
      "Epoch 248, global step 15936: 'train_loss' was not in top 1\n",
      "Epoch 249, global step 16000: 'train_loss' was not in top 1\n",
      "Epoch 250, global step 16064: 'train_loss' was not in top 1\n",
      "Epoch 251, global step 16128: 'train_loss' was not in top 1\n",
      "Epoch 252, global step 16192: 'train_loss' was not in top 1\n",
      "Epoch 253, global step 16256: 'train_loss' was not in top 1\n",
      "Epoch 254, global step 16320: 'train_loss' was not in top 1\n",
      "Epoch 255, global step 16384: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=256` reached.\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 247.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 195.84it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.84it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 221.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 271.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 228.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 253.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.94it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 214.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 250.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 251.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 262.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.36it/s]\n",
      "\n",
      "Running evaluation: 0it [00:00, ?it/s]\u001b[A/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "Running evaluation: 7it [00:00, 162.57it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/common.py:262: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  return pd.Period(val, freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/time_feature/_base.py:243: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = to_offset(freq_str)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                             | Out sizes        \n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 432 K  | [[1, 1], [1, 1], [1, 72, 5], [1, 72, 370], [1, 72, 370], [1, 24, 5]] | [1, 100, 24, 370]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "432 K     Trainable params\n",
      "0         Non-trainable params\n",
      "432 K     Total params\n",
      "1.729     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f286588046ec4d25bd3e71198a3a4013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41410 (best 0.41410), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.30522 (best 0.30522), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.11737 (best 0.11737), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.07334 (best 0.07334), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.06130 (best 0.06130), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.05290 (best 0.05290), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.04940 (best 0.04940), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.04809 (best 0.04809), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.04619 (best 0.04619), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.04516 (best 0.04516), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.04366 (best 0.04366), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.04333 (best 0.04333), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.04259 (best 0.04259), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.04198 (best 0.04198), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' reached 0.04136 (best 0.04136), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=14-step=960.ckpt' as top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.04046 (best 0.04046), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' was not in top 1\n",
      "Epoch 17, global step 1152: 'train_loss' reached 0.03960 (best 0.03960), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=17-step=1152.ckpt' as top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.03911 (best 0.03911), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' reached 0.03910 (best 0.03910), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=19-step=1280.ckpt' as top 1\n",
      "Epoch 20, global step 1344: 'train_loss' reached 0.03806 (best 0.03806), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=20-step=1344.ckpt' as top 1\n",
      "Epoch 21, global step 1408: 'train_loss' was not in top 1\n",
      "Epoch 22, global step 1472: 'train_loss' was not in top 1\n",
      "Epoch 23, global step 1536: 'train_loss' reached 0.03762 (best 0.03762), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=23-step=1536.ckpt' as top 1\n",
      "Epoch 24, global step 1600: 'train_loss' reached 0.03760 (best 0.03760), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=24-step=1600.ckpt' as top 1\n",
      "Epoch 25, global step 1664: 'train_loss' reached 0.03748 (best 0.03748), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=25-step=1664.ckpt' as top 1\n",
      "Epoch 26, global step 1728: 'train_loss' was not in top 1\n",
      "Epoch 27, global step 1792: 'train_loss' reached 0.03686 (best 0.03686), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=27-step=1792.ckpt' as top 1\n",
      "Epoch 28, global step 1856: 'train_loss' reached 0.03670 (best 0.03670), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=28-step=1856.ckpt' as top 1\n",
      "Epoch 29, global step 1920: 'train_loss' reached 0.03661 (best 0.03661), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=29-step=1920.ckpt' as top 1\n",
      "Epoch 30, global step 1984: 'train_loss' reached 0.03654 (best 0.03654), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=30-step=1984.ckpt' as top 1\n",
      "Epoch 31, global step 2048: 'train_loss' reached 0.03557 (best 0.03557), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=31-step=2048.ckpt' as top 1\n",
      "Epoch 32, global step 2112: 'train_loss' was not in top 1\n",
      "Epoch 33, global step 2176: 'train_loss' was not in top 1\n",
      "Epoch 34, global step 2240: 'train_loss' reached 0.03520 (best 0.03520), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=34-step=2240.ckpt' as top 1\n",
      "Epoch 35, global step 2304: 'train_loss' was not in top 1\n",
      "Epoch 36, global step 2368: 'train_loss' was not in top 1\n",
      "Epoch 37, global step 2432: 'train_loss' was not in top 1\n",
      "Epoch 38, global step 2496: 'train_loss' reached 0.03463 (best 0.03463), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=38-step=2496.ckpt' as top 1\n",
      "Epoch 39, global step 2560: 'train_loss' was not in top 1\n",
      "Epoch 40, global step 2624: 'train_loss' reached 0.03457 (best 0.03457), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=40-step=2624.ckpt' as top 1\n",
      "Epoch 41, global step 2688: 'train_loss' was not in top 1\n",
      "Epoch 42, global step 2752: 'train_loss' reached 0.03402 (best 0.03402), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=42-step=2752.ckpt' as top 1\n",
      "Epoch 43, global step 2816: 'train_loss' reached 0.03392 (best 0.03392), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=43-step=2816.ckpt' as top 1\n",
      "Epoch 44, global step 2880: 'train_loss' was not in top 1\n",
      "Epoch 45, global step 2944: 'train_loss' reached 0.03387 (best 0.03387), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=45-step=2944.ckpt' as top 1\n",
      "Epoch 46, global step 3008: 'train_loss' reached 0.03373 (best 0.03373), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=46-step=3008.ckpt' as top 1\n",
      "Epoch 47, global step 3072: 'train_loss' was not in top 1\n",
      "Epoch 48, global step 3136: 'train_loss' was not in top 1\n",
      "Epoch 49, global step 3200: 'train_loss' reached 0.03358 (best 0.03358), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=49-step=3200.ckpt' as top 1\n",
      "Epoch 50, global step 3264: 'train_loss' was not in top 1\n",
      "Epoch 51, global step 3328: 'train_loss' was not in top 1\n",
      "Epoch 52, global step 3392: 'train_loss' reached 0.03343 (best 0.03343), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=52-step=3392.ckpt' as top 1\n",
      "Epoch 53, global step 3456: 'train_loss' reached 0.03321 (best 0.03321), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=53-step=3456.ckpt' as top 1\n",
      "Epoch 54, global step 3520: 'train_loss' was not in top 1\n",
      "Epoch 55, global step 3584: 'train_loss' reached 0.03284 (best 0.03284), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=55-step=3584.ckpt' as top 1\n",
      "Epoch 56, global step 3648: 'train_loss' was not in top 1\n",
      "Epoch 57, global step 3712: 'train_loss' was not in top 1\n",
      "Epoch 58, global step 3776: 'train_loss' was not in top 1\n",
      "Epoch 59, global step 3840: 'train_loss' was not in top 1\n",
      "Epoch 60, global step 3904: 'train_loss' reached 0.03235 (best 0.03235), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=60-step=3904.ckpt' as top 1\n",
      "Epoch 61, global step 3968: 'train_loss' was not in top 1\n",
      "Epoch 62, global step 4032: 'train_loss' was not in top 1\n",
      "Epoch 63, global step 4096: 'train_loss' was not in top 1\n",
      "Epoch 64, global step 4160: 'train_loss' was not in top 1\n",
      "Epoch 65, global step 4224: 'train_loss' reached 0.03180 (best 0.03180), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=65-step=4224.ckpt' as top 1\n",
      "Epoch 66, global step 4288: 'train_loss' was not in top 1\n",
      "Epoch 67, global step 4352: 'train_loss' was not in top 1\n",
      "Epoch 68, global step 4416: 'train_loss' reached 0.03140 (best 0.03140), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=68-step=4416.ckpt' as top 1\n",
      "Epoch 69, global step 4480: 'train_loss' was not in top 1\n",
      "Epoch 70, global step 4544: 'train_loss' was not in top 1\n",
      "Epoch 71, global step 4608: 'train_loss' was not in top 1\n",
      "Epoch 72, global step 4672: 'train_loss' was not in top 1\n",
      "Epoch 73, global step 4736: 'train_loss' was not in top 1\n",
      "Epoch 74, global step 4800: 'train_loss' was not in top 1\n",
      "Epoch 75, global step 4864: 'train_loss' was not in top 1\n",
      "Epoch 76, global step 4928: 'train_loss' was not in top 1\n",
      "Epoch 77, global step 4992: 'train_loss' reached 0.03118 (best 0.03118), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=77-step=4992.ckpt' as top 1\n",
      "Epoch 78, global step 5056: 'train_loss' was not in top 1\n",
      "Epoch 79, global step 5120: 'train_loss' was not in top 1\n",
      "Epoch 80, global step 5184: 'train_loss' was not in top 1\n",
      "Epoch 81, global step 5248: 'train_loss' was not in top 1\n",
      "Epoch 82, global step 5312: 'train_loss' was not in top 1\n",
      "Epoch 83, global step 5376: 'train_loss' reached 0.03100 (best 0.03100), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=83-step=5376.ckpt' as top 1\n",
      "Epoch 84, global step 5440: 'train_loss' was not in top 1\n",
      "Epoch 85, global step 5504: 'train_loss' was not in top 1\n",
      "Epoch 86, global step 5568: 'train_loss' was not in top 1\n",
      "Epoch 87, global step 5632: 'train_loss' reached 0.03077 (best 0.03077), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=87-step=5632.ckpt' as top 1\n",
      "Epoch 88, global step 5696: 'train_loss' was not in top 1\n",
      "Epoch 89, global step 5760: 'train_loss' reached 0.03021 (best 0.03021), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=89-step=5760.ckpt' as top 1\n",
      "Epoch 90, global step 5824: 'train_loss' was not in top 1\n",
      "Epoch 91, global step 5888: 'train_loss' was not in top 1\n",
      "Epoch 92, global step 5952: 'train_loss' was not in top 1\n",
      "Epoch 93, global step 6016: 'train_loss' was not in top 1\n",
      "Epoch 94, global step 6080: 'train_loss' was not in top 1\n",
      "Epoch 95, global step 6144: 'train_loss' was not in top 1\n",
      "Epoch 96, global step 6208: 'train_loss' was not in top 1\n",
      "Epoch 97, global step 6272: 'train_loss' was not in top 1\n",
      "Epoch 98, global step 6336: 'train_loss' was not in top 1\n",
      "Epoch 99, global step 6400: 'train_loss' was not in top 1\n",
      "Epoch 100, global step 6464: 'train_loss' was not in top 1\n",
      "Epoch 101, global step 6528: 'train_loss' reached 0.02993 (best 0.02993), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=101-step=6528.ckpt' as top 1\n",
      "Epoch 102, global step 6592: 'train_loss' was not in top 1\n",
      "Epoch 103, global step 6656: 'train_loss' was not in top 1\n",
      "Epoch 104, global step 6720: 'train_loss' was not in top 1\n",
      "Epoch 105, global step 6784: 'train_loss' was not in top 1\n",
      "Epoch 106, global step 6848: 'train_loss' reached 0.02981 (best 0.02981), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=106-step=6848.ckpt' as top 1\n",
      "Epoch 107, global step 6912: 'train_loss' reached 0.02974 (best 0.02974), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=107-step=6912.ckpt' as top 1\n",
      "Epoch 108, global step 6976: 'train_loss' was not in top 1\n",
      "Epoch 109, global step 7040: 'train_loss' reached 0.02956 (best 0.02956), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=109-step=7040.ckpt' as top 1\n",
      "Epoch 110, global step 7104: 'train_loss' was not in top 1\n",
      "Epoch 111, global step 7168: 'train_loss' was not in top 1\n",
      "Epoch 112, global step 7232: 'train_loss' was not in top 1\n",
      "Epoch 113, global step 7296: 'train_loss' was not in top 1\n",
      "Epoch 114, global step 7360: 'train_loss' reached 0.02955 (best 0.02955), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=114-step=7360.ckpt' as top 1\n",
      "Epoch 115, global step 7424: 'train_loss' was not in top 1\n",
      "Epoch 116, global step 7488: 'train_loss' was not in top 1\n",
      "Epoch 117, global step 7552: 'train_loss' was not in top 1\n",
      "Epoch 118, global step 7616: 'train_loss' was not in top 1\n",
      "Epoch 119, global step 7680: 'train_loss' was not in top 1\n",
      "Epoch 120, global step 7744: 'train_loss' reached 0.02950 (best 0.02950), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=120-step=7744.ckpt' as top 1\n",
      "Epoch 121, global step 7808: 'train_loss' reached 0.02947 (best 0.02947), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=121-step=7808.ckpt' as top 1\n",
      "Epoch 122, global step 7872: 'train_loss' reached 0.02931 (best 0.02931), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=122-step=7872.ckpt' as top 1\n",
      "Epoch 123, global step 7936: 'train_loss' was not in top 1\n",
      "Epoch 124, global step 8000: 'train_loss' was not in top 1\n",
      "Epoch 125, global step 8064: 'train_loss' was not in top 1\n",
      "Epoch 126, global step 8128: 'train_loss' was not in top 1\n",
      "Epoch 127, global step 8192: 'train_loss' was not in top 1\n",
      "Epoch 128, global step 8256: 'train_loss' was not in top 1\n",
      "Epoch 129, global step 8320: 'train_loss' was not in top 1\n",
      "Epoch 130, global step 8384: 'train_loss' was not in top 1\n",
      "Epoch 131, global step 8448: 'train_loss' reached 0.02909 (best 0.02909), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=131-step=8448.ckpt' as top 1\n",
      "Epoch 132, global step 8512: 'train_loss' was not in top 1\n",
      "Epoch 133, global step 8576: 'train_loss' was not in top 1\n",
      "Epoch 134, global step 8640: 'train_loss' was not in top 1\n",
      "Epoch 135, global step 8704: 'train_loss' was not in top 1\n",
      "Epoch 136, global step 8768: 'train_loss' was not in top 1\n",
      "Epoch 137, global step 8832: 'train_loss' was not in top 1\n",
      "Epoch 138, global step 8896: 'train_loss' was not in top 1\n",
      "Epoch 139, global step 8960: 'train_loss' was not in top 1\n",
      "Epoch 140, global step 9024: 'train_loss' was not in top 1\n",
      "Epoch 141, global step 9088: 'train_loss' was not in top 1\n",
      "Epoch 142, global step 9152: 'train_loss' was not in top 1\n",
      "Epoch 143, global step 9216: 'train_loss' was not in top 1\n",
      "Epoch 144, global step 9280: 'train_loss' reached 0.02892 (best 0.02892), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=144-step=9280.ckpt' as top 1\n",
      "Epoch 145, global step 9344: 'train_loss' was not in top 1\n",
      "Epoch 146, global step 9408: 'train_loss' reached 0.02847 (best 0.02847), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=146-step=9408.ckpt' as top 1\n",
      "Epoch 147, global step 9472: 'train_loss' was not in top 1\n",
      "Epoch 148, global step 9536: 'train_loss' was not in top 1\n",
      "Epoch 149, global step 9600: 'train_loss' was not in top 1\n",
      "Epoch 150, global step 9664: 'train_loss' was not in top 1\n",
      "Epoch 151, global step 9728: 'train_loss' was not in top 1\n",
      "Epoch 152, global step 9792: 'train_loss' was not in top 1\n",
      "Epoch 153, global step 9856: 'train_loss' was not in top 1\n",
      "Epoch 154, global step 9920: 'train_loss' was not in top 1\n",
      "Epoch 155, global step 9984: 'train_loss' was not in top 1\n",
      "Epoch 156, global step 10048: 'train_loss' was not in top 1\n",
      "Epoch 157, global step 10112: 'train_loss' was not in top 1\n",
      "Epoch 158, global step 10176: 'train_loss' was not in top 1\n",
      "Epoch 159, global step 10240: 'train_loss' reached 0.02831 (best 0.02831), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=159-step=10240.ckpt' as top 1\n",
      "Epoch 160, global step 10304: 'train_loss' was not in top 1\n",
      "Epoch 161, global step 10368: 'train_loss' was not in top 1\n",
      "Epoch 162, global step 10432: 'train_loss' was not in top 1\n",
      "Epoch 163, global step 10496: 'train_loss' was not in top 1\n",
      "Epoch 164, global step 10560: 'train_loss' was not in top 1\n",
      "Epoch 165, global step 10624: 'train_loss' was not in top 1\n",
      "Epoch 166, global step 10688: 'train_loss' was not in top 1\n",
      "Epoch 167, global step 10752: 'train_loss' was not in top 1\n",
      "Epoch 168, global step 10816: 'train_loss' was not in top 1\n",
      "Epoch 169, global step 10880: 'train_loss' was not in top 1\n",
      "Epoch 170, global step 10944: 'train_loss' was not in top 1\n",
      "Epoch 171, global step 11008: 'train_loss' was not in top 1\n",
      "Epoch 172, global step 11072: 'train_loss' was not in top 1\n",
      "Epoch 173, global step 11136: 'train_loss' was not in top 1\n",
      "Epoch 174, global step 11200: 'train_loss' was not in top 1\n",
      "Epoch 175, global step 11264: 'train_loss' was not in top 1\n",
      "Epoch 176, global step 11328: 'train_loss' was not in top 1\n",
      "Epoch 177, global step 11392: 'train_loss' was not in top 1\n",
      "Epoch 178, global step 11456: 'train_loss' was not in top 1\n",
      "Epoch 179, global step 11520: 'train_loss' was not in top 1\n",
      "Epoch 180, global step 11584: 'train_loss' was not in top 1\n",
      "Epoch 181, global step 11648: 'train_loss' was not in top 1\n",
      "Epoch 182, global step 11712: 'train_loss' was not in top 1\n",
      "Epoch 183, global step 11776: 'train_loss' was not in top 1\n",
      "Epoch 184, global step 11840: 'train_loss' was not in top 1\n",
      "Epoch 185, global step 11904: 'train_loss' was not in top 1\n",
      "Epoch 186, global step 11968: 'train_loss' was not in top 1\n",
      "Epoch 187, global step 12032: 'train_loss' was not in top 1\n",
      "Epoch 188, global step 12096: 'train_loss' was not in top 1\n",
      "Epoch 189, global step 12160: 'train_loss' was not in top 1\n",
      "Epoch 190, global step 12224: 'train_loss' was not in top 1\n",
      "Epoch 191, global step 12288: 'train_loss' was not in top 1\n",
      "Epoch 192, global step 12352: 'train_loss' was not in top 1\n",
      "Epoch 193, global step 12416: 'train_loss' was not in top 1\n",
      "Epoch 194, global step 12480: 'train_loss' was not in top 1\n",
      "Epoch 195, global step 12544: 'train_loss' was not in top 1\n",
      "Epoch 196, global step 12608: 'train_loss' was not in top 1\n",
      "Epoch 197, global step 12672: 'train_loss' was not in top 1\n",
      "Epoch 198, global step 12736: 'train_loss' was not in top 1\n",
      "Epoch 199, global step 12800: 'train_loss' was not in top 1\n",
      "Epoch 200, global step 12864: 'train_loss' was not in top 1\n",
      "Epoch 201, global step 12928: 'train_loss' was not in top 1\n",
      "Epoch 202, global step 12992: 'train_loss' was not in top 1\n",
      "Epoch 203, global step 13056: 'train_loss' was not in top 1\n",
      "Epoch 204, global step 13120: 'train_loss' was not in top 1\n",
      "Epoch 205, global step 13184: 'train_loss' was not in top 1\n",
      "Epoch 206, global step 13248: 'train_loss' was not in top 1\n",
      "Epoch 207, global step 13312: 'train_loss' was not in top 1\n",
      "Epoch 208, global step 13376: 'train_loss' was not in top 1\n",
      "Epoch 209, global step 13440: 'train_loss' was not in top 1\n",
      "Epoch 210, global step 13504: 'train_loss' was not in top 1\n",
      "Epoch 211, global step 13568: 'train_loss' was not in top 1\n",
      "Epoch 212, global step 13632: 'train_loss' was not in top 1\n",
      "Epoch 213, global step 13696: 'train_loss' was not in top 1\n",
      "Epoch 214, global step 13760: 'train_loss' reached 0.02822 (best 0.02822), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=214-step=13760.ckpt' as top 1\n",
      "Epoch 215, global step 13824: 'train_loss' was not in top 1\n",
      "Epoch 216, global step 13888: 'train_loss' was not in top 1\n",
      "Epoch 217, global step 13952: 'train_loss' was not in top 1\n",
      "Epoch 218, global step 14016: 'train_loss' was not in top 1\n",
      "Epoch 219, global step 14080: 'train_loss' was not in top 1\n",
      "Epoch 220, global step 14144: 'train_loss' was not in top 1\n",
      "Epoch 221, global step 14208: 'train_loss' was not in top 1\n",
      "Epoch 222, global step 14272: 'train_loss' was not in top 1\n",
      "Epoch 223, global step 14336: 'train_loss' was not in top 1\n",
      "Epoch 224, global step 14400: 'train_loss' was not in top 1\n",
      "Epoch 225, global step 14464: 'train_loss' was not in top 1\n",
      "Epoch 226, global step 14528: 'train_loss' was not in top 1\n",
      "Epoch 227, global step 14592: 'train_loss' was not in top 1\n",
      "Epoch 228, global step 14656: 'train_loss' was not in top 1\n",
      "Epoch 229, global step 14720: 'train_loss' was not in top 1\n",
      "Epoch 230, global step 14784: 'train_loss' was not in top 1\n",
      "Epoch 231, global step 14848: 'train_loss' was not in top 1\n",
      "Epoch 232, global step 14912: 'train_loss' was not in top 1\n",
      "Epoch 233, global step 14976: 'train_loss' was not in top 1\n",
      "Epoch 234, global step 15040: 'train_loss' was not in top 1\n",
      "Epoch 235, global step 15104: 'train_loss' reached 0.02794 (best 0.02794), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=235-step=15104.ckpt' as top 1\n",
      "Epoch 236, global step 15168: 'train_loss' was not in top 1\n",
      "Epoch 237, global step 15232: 'train_loss' was not in top 1\n",
      "Epoch 238, global step 15296: 'train_loss' was not in top 1\n",
      "Epoch 239, global step 15360: 'train_loss' was not in top 1\n",
      "Epoch 240, global step 15424: 'train_loss' was not in top 1\n",
      "Epoch 241, global step 15488: 'train_loss' was not in top 1\n",
      "Epoch 242, global step 15552: 'train_loss' was not in top 1\n",
      "Epoch 243, global step 15616: 'train_loss' was not in top 1\n",
      "Epoch 244, global step 15680: 'train_loss' was not in top 1\n",
      "Epoch 245, global step 15744: 'train_loss' was not in top 1\n",
      "Epoch 246, global step 15808: 'train_loss' was not in top 1\n",
      "Epoch 247, global step 15872: 'train_loss' was not in top 1\n",
      "Epoch 248, global step 15936: 'train_loss' was not in top 1\n",
      "Epoch 249, global step 16000: 'train_loss' was not in top 1\n",
      "Epoch 250, global step 16064: 'train_loss' was not in top 1\n",
      "Epoch 251, global step 16128: 'train_loss' was not in top 1\n",
      "Epoch 252, global step 16192: 'train_loss' was not in top 1\n",
      "Epoch 253, global step 16256: 'train_loss' was not in top 1\n",
      "Epoch 254, global step 16320: 'train_loss' was not in top 1\n",
      "Epoch 255, global step 16384: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=256` reached.\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 242.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.97it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 296.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 208.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.23it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 198.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 227.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 253.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 208.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.97it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 258.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 266.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 299.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.77it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 254.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 220.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 114.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 226.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 220.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 214.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.77it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 152.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 44.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.56it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 250.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 273.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.43it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 249.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.94it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 251.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 272.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 221.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 269.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.24it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 265.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 248.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.43it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.23it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 247.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 261.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 254.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 246.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 255.31it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 266.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 199.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.68it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: UserWarning: Warning: converting a masked element to nan.\n",
      "  return arr.astype(dtype, copy=True)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:531: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[\"NRMSE\"] = totals[\"RMSE\"] / totals[\"abs_target_mean\"]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:532: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[\"ND\"] = totals[\"abs_error\"] / totals[\"abs_target_sum\"]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:536: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[f\"QuantileLoss[{quantile}]\"] / totals[\"abs_target_sum\"]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 205.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 178.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 270.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 296.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 199.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 249.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 198.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 262.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.24it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 242.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 272.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 196.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.71it/s]\n",
      "\n",
      "Running evaluation: 0it [00:00, ?it/s]\u001b[A/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "Running evaluation: 7it [00:00, 165.74it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/common.py:262: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  return pd.Period(val, freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:113: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  timestamp + len(data[FieldName.TARGET]) - 1,\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:242: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  index=pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:242: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  index=pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:187: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:187: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:198: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:198: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  pd.period_range(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                         | Out sizes      \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 73.4 K | [[1, 1], [1, 1], [1, 90, 4], [1, 90, 8], [1, 90, 8], [1, 30, 4]] | [1, 100, 30, 8]\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "73.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "73.4 K    Total params\n",
      "0.294     Total estimated model params size (MB)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/feature.py:364: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  index = pd.period_range(start, periods=length, freq=start.freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/feature.py:364: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  index = pd.period_range(start, periods=length, freq=start.freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/split.py:150: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  entry[self.start_field] + idx + self.lead_time\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1092ffb7814a5683adca0f8f9211c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41521 (best 0.41521), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.28120 (best 0.28120), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.08373 (best 0.08373), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.04662 (best 0.04662), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.03065 (best 0.03065), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.02148 (best 0.02148), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.01611 (best 0.01611), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.01328 (best 0.01328), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.01189 (best 0.01189), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.01063 (best 0.01063), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.01004 (best 0.01004), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.00924 (best 0.00924), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.00840 (best 0.00840), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.00801 (best 0.00801), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' was not in top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.00759 (best 0.00759), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' was not in top 1\n",
      "Epoch 17, global step 1152: 'train_loss' was not in top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.00706 (best 0.00706), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' was not in top 1\n",
      "Epoch 20, global step 1344: 'train_loss' was not in top 1\n",
      "Epoch 21, global step 1408: 'train_loss' reached 0.00657 (best 0.00657), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=21-step=1408.ckpt' as top 1\n",
      "Epoch 22, global step 1472: 'train_loss' reached 0.00642 (best 0.00642), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=22-step=1472.ckpt' as top 1\n",
      "Epoch 23, global step 1536: 'train_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"solar_nips\": [],\n",
    "    \"electricity_nips\": [],\n",
    "    \"exchange_rate_nips\": []\n",
    "}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    dataset = prepare_dataset(dataset_name)\n",
    "    predictor = prepare_predictor(dataset)\n",
    "    forecasts, targets, agg_metric = prepare_metrics(dataset, predictor)\n",
    "    datasets[dataset_name] = {\n",
    "        \"dataset\": dataset,\n",
    "        \"predictor\": predictor,\n",
    "        \"forecasts\": forecasts,\n",
    "        \"targets\": targets,\n",
    "        \"agg_metric\": agg_metric\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"solar_nips\"][\"dataset\"], datasets[\"solar_nips\"][\"forecasts\"],\n",
    "    datasets[\"solar_nips\"][\"targets\"], datasets[\"solar_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"electricity_nips\"][\"dataset\"], datasets[\"electricity_nips\"][\"forecasts\"],\n",
    "    datasets[\"electricity_nips\"][\"targets\"], datasets[\"electricity_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"exchange_rate_nips\"][\"dataset\"], datasets[\"exchange_rate_nips\"][\"forecasts\"],\n",
    "    datasets[\"exchange_rate_nips\"][\"targets\"], datasets[\"exchange_rate_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
