{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/nbukhanchenko/trans-timegrad\n",
      "  Cloning https://github.com/nbukhanchenko/trans-timegrad to /private/var/folders/tx/_dy4v0xn2wvgss_0qgrgrfs40000gn/T/pip-req-build-07sfebvt\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nbukhanchenko/trans-timegrad /private/var/folders/tx/_dy4v0xn2wvgss_0qgrgrfs40000gn/T/pip-req-build-07sfebvt\n",
      "  Resolved https://github.com/nbukhanchenko/trans-timegrad to commit c6bc224ecf9515a3a06dda8cfd08c1169b38a339\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting diffusers==0.25.1 (from trans-timegrad==1.0)\n",
      "  Using cached diffusers-0.25.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting gluonts==0.14.3 (from trans-timegrad==1.0)\n",
      "  Using cached gluonts-0.14.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting holidays (from trans-timegrad==1.0)\n",
      "  Downloading holidays-0.44-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting torch>=1.8.0 (from trans-timegrad==1.0)\n",
      "  Using cached torch-2.2.1-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting lightning==2.1.3 (from trans-timegrad==1.0)\n",
      "  Using cached lightning-2.1.3-py3-none-any.whl.metadata (56 kB)\n",
      "Collecting matplotlib (from trans-timegrad==1.0)\n",
      "  Using cached matplotlib-3.8.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting numpy==1.23.5 (from trans-timegrad==1.0)\n",
      "  Using cached numpy-1.23.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting pandas==2.2.0 (from trans-timegrad==1.0)\n",
      "  Using cached pandas-2.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting protobuf~=3.20.3 (from trans-timegrad==1.0)\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting seaborn==0.13.1 (from trans-timegrad==1.0)\n",
      "  Using cached seaborn-0.13.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata (from diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Downloading importlib_metadata-7.0.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting filelock (from diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub>=0.20.2 (from diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.3.1 (from diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting Pillow (from diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached pillow-10.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting pydantic<3,>=1.7 (from gluonts==0.14.3->trans-timegrad==1.0)\n",
      "  Using cached pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "Collecting tqdm~=4.23 (from gluonts==0.14.3->trans-timegrad==1.0)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting toolz~=0.10 (from gluonts==0.14.3->trans-timegrad==1.0)\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting typing-extensions~=4.0 (from gluonts==0.14.3->trans-timegrad==1.0)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting PyYAML<8.0,>=5.4 (from lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting fsspec<2025.0,>=2022.5.0 (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting packaging<25.0,>=20.0 (from lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pytorch-lightning (from lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached pytorch_lightning-2.2.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas==2.2.0->trans-timegrad==1.0)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas==2.2.0->trans-timegrad==1.0)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.0->trans-timegrad==1.0)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->trans-timegrad==1.0)\n",
      "  Using cached contourpy-1.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->trans-timegrad==1.0)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->trans-timegrad==1.0)\n",
      "  Using cached fonttools-4.49.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (159 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->trans-timegrad==1.0)\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->trans-timegrad==1.0)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting sympy (from torch>=1.8.0->trans-timegrad==1.0)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.8.0->trans-timegrad==1.0)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.8.0->trans-timegrad==1.0)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting setuptools (from lightning-utilities<2.0,>=0.8.0->lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached setuptools-69.1.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.7->gluonts==0.14.3->trans-timegrad==1.0)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic<3,>=1.7->gluonts==0.14.3->trans-timegrad==1.0)\n",
      "  Using cached pydantic_core-2.16.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.0->trans-timegrad==1.0)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata->diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.8.0->trans-timegrad==1.0)\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->diffusers==0.25.1->trans-timegrad==1.0)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.8.0->trans-timegrad==1.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.1.3->trans-timegrad==1.0)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Using cached diffusers-0.25.1-py3-none-any.whl (1.8 MB)\n",
      "Using cached gluonts-0.14.3-py3-none-any.whl (1.5 MB)\n",
      "Using cached lightning-2.1.3-py3-none-any.whl (2.0 MB)\n",
      "Using cached numpy-1.23.5-cp311-cp311-macosx_11_0_arm64.whl (13.3 MB)\n",
      "Using cached pandas-2.2.0-cp311-cp311-macosx_11_0_arm64.whl (11.8 MB)\n",
      "Using cached seaborn-0.13.1-py3-none-any.whl (294 kB)\n",
      "Using cached matplotlib-3.8.3-cp311-cp311-macosx_11_0_arm64.whl (7.5 MB)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Using cached torch-2.2.1-cp311-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "Downloading holidays-0.44-py3-none-any.whl (922 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.7/922.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached contourpy-1.2.0-cp311-cp311-macosx_11_0_arm64.whl (243 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.49.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl (66 kB)\n",
      "Using cached lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached pillow-10.2.0-cp311-cp311-macosx_11_0_arm64.whl (3.3 MB)\n",
      "Using cached pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "Using cached pydantic_core-2.16.3-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Using cached regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "Using cached safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl (393 kB)\n",
      "Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Using cached torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading importlib_metadata-7.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl (387 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Using cached setuptools-69.1.1-py3-none-any.whl (819 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "Building wheels for collected packages: trans-timegrad\n",
      "  Building wheel for trans-timegrad (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for trans-timegrad: filename=trans_timegrad-1.0-py3-none-any.whl size=24034 sha256=7be9e4adfcef05c132eadc77440b61bf52b485b63bddc2bc0c407252aa038b95\n",
      "  Stored in directory: /private/var/folders/tx/_dy4v0xn2wvgss_0qgrgrfs40000gn/T/pip-ephem-wheel-cache-cbf_iqa3/wheels/20/4f/64/ec8ca9f731fe5f497555f0bab75d6acb17fd706467fbf16f2d\n",
      "Successfully built trans-timegrad\n",
      "Installing collected packages: pytz, mpmath, zipp, urllib3, tzdata, typing-extensions, tqdm, toolz, sympy, six, setuptools, safetensors, regex, PyYAML, pyparsing, protobuf, Pillow, packaging, numpy, networkx, multidict, MarkupSafe, kiwisolver, idna, fsspec, frozenlist, fonttools, filelock, cycler, charset-normalizer, certifi, attrs, annotated-types, yarl, requests, python-dateutil, pydantic-core, lightning-utilities, jinja2, importlib-metadata, contourpy, aiosignal, torch, pydantic, pandas, matplotlib, huggingface-hub, holidays, aiohttp, torchmetrics, seaborn, gluonts, diffusers, pytorch-lightning, lightning, trans-timegrad\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2024.1\n",
      "    Uninstalling pytz-2024.1:\n",
      "      Successfully uninstalled pytz-2024.1\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.17.0\n",
      "    Uninstalling zipp-3.17.0:\n",
      "      Successfully uninstalled zipp-3.17.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2024.1\n",
      "    Uninstalling tzdata-2024.1:\n",
      "      Successfully uninstalled tzdata-2024.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: toolz\n",
      "    Found existing installation: toolz 0.12.1\n",
      "    Uninstalling toolz-0.12.1:\n",
      "      Successfully uninstalled toolz-0.12.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.1.1\n",
      "    Uninstalling setuptools-69.1.1:\n",
      "      Successfully uninstalled setuptools-69.1.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.2\n",
      "    Uninstalling safetensors-0.4.2:\n",
      "      Successfully uninstalled safetensors-0.4.2\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.12.25\n",
      "    Uninstalling regex-2023.12.25:\n",
      "      Successfully uninstalled regex-2023.12.25\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.2\n",
      "    Uninstalling pyparsing-3.1.2:\n",
      "      Successfully uninstalled pyparsing-3.1.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.2.1\n",
      "    Uninstalling networkx-3.2.1:\n",
      "      Successfully uninstalled networkx-3.2.1\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.0.5\n",
      "    Uninstalling multidict-6.0.5:\n",
      "      Successfully uninstalled multidict-6.0.5\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.5\n",
      "    Uninstalling kiwisolver-1.4.5:\n",
      "      Successfully uninstalled kiwisolver-1.4.5\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.1\n",
      "    Uninstalling frozenlist-1.4.1:\n",
      "      Successfully uninstalled frozenlist-1.4.1\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.49.0\n",
      "    Uninstalling fonttools-4.49.0:\n",
      "      Successfully uninstalled fonttools-4.49.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.12.1\n",
      "    Uninstalling cycler-0.12.1:\n",
      "      Successfully uninstalled cycler-0.12.1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.2.0\n",
      "    Uninstalling attrs-23.2.0:\n",
      "      Successfully uninstalled attrs-23.2.0\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.6.0\n",
      "    Uninstalling annotated-types-0.6.0:\n",
      "      Successfully uninstalled annotated-types-0.6.0\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.9.4\n",
      "    Uninstalling yarl-1.9.4:\n",
      "      Successfully uninstalled yarl-1.9.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.16.3\n",
      "    Uninstalling pydantic_core-2.16.3:\n",
      "      Successfully uninstalled pydantic_core-2.16.3\n",
      "  Attempting uninstall: lightning-utilities\n",
      "    Found existing installation: lightning-utilities 0.10.1\n",
      "    Uninstalling lightning-utilities-0.10.1:\n",
      "      Successfully uninstalled lightning-utilities-0.10.1\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.3\n",
      "    Uninstalling Jinja2-3.1.3:\n",
      "      Successfully uninstalled Jinja2-3.1.3\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 7.0.1\n",
      "    Uninstalling importlib-metadata-7.0.1:\n",
      "      Successfully uninstalled importlib-metadata-7.0.1\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.2.0\n",
      "    Uninstalling contourpy-1.2.0:\n",
      "      Successfully uninstalled contourpy-1.2.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.1\n",
      "    Uninstalling aiosignal-1.3.1:\n",
      "      Successfully uninstalled aiosignal-1.3.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1\n",
      "    Uninstalling torch-2.2.1:\n",
      "      Successfully uninstalled torch-2.2.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.6.3\n",
      "    Uninstalling pydantic-2.6.3:\n",
      "      Successfully uninstalled pydantic-2.6.3\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.0\n",
      "    Uninstalling pandas-2.2.0:\n",
      "      Successfully uninstalled pandas-2.2.0\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.3\n",
      "    Uninstalling matplotlib-3.8.3:\n",
      "      Successfully uninstalled matplotlib-3.8.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.21.4\n",
      "    Uninstalling huggingface-hub-0.21.4:\n",
      "      Successfully uninstalled huggingface-hub-0.21.4\n",
      "  Attempting uninstall: holidays\n",
      "    Found existing installation: holidays 0.43\n",
      "    Uninstalling holidays-0.43:\n",
      "      Successfully uninstalled holidays-0.43\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.3\n",
      "    Uninstalling aiohttp-3.9.3:\n",
      "      Successfully uninstalled aiohttp-3.9.3\n",
      "  Attempting uninstall: torchmetrics\n",
      "    Found existing installation: torchmetrics 1.3.1\n",
      "    Uninstalling torchmetrics-1.3.1:\n",
      "      Successfully uninstalled torchmetrics-1.3.1\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.13.1\n",
      "    Uninstalling seaborn-0.13.1:\n",
      "      Successfully uninstalled seaborn-0.13.1\n",
      "  Attempting uninstall: gluonts\n",
      "    Found existing installation: gluonts 0.14.3\n",
      "    Uninstalling gluonts-0.14.3:\n",
      "      Successfully uninstalled gluonts-0.14.3\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.25.1\n",
      "    Uninstalling diffusers-0.25.1:\n",
      "      Successfully uninstalled diffusers-0.25.1\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 2.2.1\n",
      "    Uninstalling pytorch-lightning-2.2.1:\n",
      "      Successfully uninstalled pytorch-lightning-2.2.1\n",
      "  Attempting uninstall: lightning\n",
      "    Found existing installation: lightning 2.1.3\n",
      "    Uninstalling lightning-2.1.3:\n",
      "      Successfully uninstalled lightning-2.1.3\n",
      "  Attempting uninstall: trans-timegrad\n",
      "    Found existing installation: trans-timegrad 1.0\n",
      "    Uninstalling trans-timegrad-1.0:\n",
      "      Successfully uninstalled trans-timegrad-1.0\n",
      "Successfully installed MarkupSafe-2.1.5 Pillow-10.2.0 PyYAML-6.0.1 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 contourpy-1.2.0 cycler-0.12.1 diffusers-0.25.1 filelock-3.13.1 fonttools-4.49.0 frozenlist-1.4.1 fsspec-2024.2.0 gluonts-0.14.3 holidays-0.44 huggingface-hub-0.21.4 idna-3.6 importlib-metadata-7.0.2 jinja2-3.1.3 kiwisolver-1.4.5 lightning-2.1.3 lightning-utilities-0.10.1 matplotlib-3.8.3 mpmath-1.3.0 multidict-6.0.5 networkx-3.2.1 numpy-1.23.5 packaging-23.2 pandas-2.2.0 protobuf-3.20.3 pydantic-2.6.3 pydantic-core-2.16.3 pyparsing-3.1.2 python-dateutil-2.9.0.post0 pytorch-lightning-2.2.1 pytz-2024.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 seaborn-0.13.1 setuptools-69.1.1 six-1.16.0 sympy-1.12 toolz-0.12.1 torch-2.2.1 torchmetrics-1.3.1 tqdm-4.66.2 trans-timegrad-1.0 typing-extensions-4.10.0 tzdata-2024.1 urllib3-2.2.1 yarl-1.9.4 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall \"git+https://github.com/nbukhanchenko/trans-timegrad\"\n",
    "# !pip install --upgrade --force-reinstall gluonts==0.14.3\n",
    "# !pip install --upgrade --force-reinstall seaborn==0.13.1\n",
    "# !pip install --upgrade --force-reinstall pandas==2.2.0\n",
    "# !pip install --upgrade --force-reinstall numpy==1.23.5\n",
    "# !pip install --upgrade --force-reinstall lightning==2.1.3\n",
    "# !pip install --upgrade --force-reinstall diffusers==0.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.dataset.repository.datasets import dataset_recipes, get_dataset\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import MultivariateEvaluator\n",
    "from diffusers import (\n",
    "    DDPMScheduler,\n",
    "    PNDMScheduler,\n",
    "    DDIMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    KDPM2DiscreteScheduler,\n",
    "    DEISMultistepScheduler,\n",
    ")\n",
    "\n",
    "from ttg.model.time_grad import TimeGradEstimator\n",
    "from ttg.dataset.repository.datasets import dataset_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_for_percentile(p):\n",
    "    return (p / 100.0) ** 0.3\n",
    "\n",
    "def plot(\n",
    "    target,\n",
    "    forecast,\n",
    "    prediction_length,\n",
    "    prediction_intervals=(50.0, 90.0),\n",
    "    color=\"g\",\n",
    "    fname=None,\n",
    "):\n",
    "    label_prefix = \"\"\n",
    "    rows = 4\n",
    "    cols = 4\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(24, 24))\n",
    "    axx = axs.ravel()\n",
    "    seq_len, target_dim = target.shape\n",
    "\n",
    "    ps = [50.0] + [\n",
    "        50.0 + f * c / 2.0 for c in prediction_intervals for f in [-1.0, +1.0]\n",
    "    ]\n",
    "\n",
    "    percentiles_sorted = sorted(set(ps))\n",
    "\n",
    "    for dim in range(0, min(rows * cols, target_dim)):\n",
    "        ax = axx[dim]\n",
    "\n",
    "        target[-2 * prediction_length :][dim].plot(ax=ax)\n",
    "\n",
    "        ps_data = [forecast.quantile(p / 100.0)[:, dim] for p in percentiles_sorted]\n",
    "        i_p50 = len(percentiles_sorted) // 2\n",
    "\n",
    "        p50_data = ps_data[i_p50]\n",
    "        p50_series = pd.Series(data=p50_data, index=forecast.index)\n",
    "        p50_series.plot(color=color, ls=\"-\", label=f\"{label_prefix}median\", ax=ax)\n",
    "\n",
    "        for i in range(len(percentiles_sorted) // 2):\n",
    "            ptile = percentiles_sorted[i]\n",
    "            alpha = alpha_for_percentile(ptile)\n",
    "            ax.fill_between(\n",
    "                forecast.index,\n",
    "                ps_data[i],\n",
    "                ps_data[-i - 1],\n",
    "                facecolor=color,\n",
    "                alpha=alpha,\n",
    "                interpolate=True,\n",
    "            )\n",
    "            # Hack to create labels for the error intervals.\n",
    "            # Doesn't actually plot anything, because we only pass a single data point\n",
    "            pd.Series(data=p50_data[:1], index=forecast.index[:1]).plot(\n",
    "                color=color,\n",
    "                alpha=alpha,\n",
    "                linewidth=10,\n",
    "                label=f\"{label_prefix}{100 - ptile * 2}%\",\n",
    "                ax=ax,\n",
    "            )\n",
    "\n",
    "    legend = [\"observations\", \"median prediction\"] + [\n",
    "        f\"{k}% prediction interval\" for k in prediction_intervals\n",
    "    ][::-1]\n",
    "    axx[0].legend(legend, loc=\"upper left\")\n",
    "\n",
    "    if fname is not None:\n",
    "        plt.savefig(fname, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "\n",
    "def prepare_dataset(dataset_name):\n",
    "    dataset = get_dataset(dataset_name, regenerate=False)\n",
    "\n",
    "    train_grouper = MultivariateGrouper(\n",
    "        max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality)\n",
    "    )\n",
    "    test_grouper = MultivariateGrouper(\n",
    "        num_test_dates=int(len(dataset.test) / len(dataset.train)),\n",
    "        max_target_dim=int(dataset.metadata.feat_static_cat[0].cardinality),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train\": train_grouper(dataset.train),\n",
    "        \"test\": test_grouper(dataset.test),\n",
    "        \"metadata\": dataset.metadata\n",
    "    }\n",
    "\n",
    "def prepare_predictor(dataset, max_epochs=256,\n",
    "                      num_train_timesteps=150, beta_start=1e-4, beta_end=0.1, beta_schedule=\"linear\",\n",
    "                      context_length_coef=3, num_layers=2, hidden_size=64, lr=3e-4, weight_decay=1e-8, dropout_rate=0.1,\n",
    "                      lags_seq=[1], num_inference_steps=149, batch_size=64, num_batches_per_epoch=64):\n",
    "#     scheduler = PNDMScheduler(\n",
    "#         num_train_timesteps=num_train_timesteps,\n",
    "#         beta_start=beta_start,\n",
    "#         beta_end=beta_end,\n",
    "#         beta_schedule=beta_schedule,\n",
    "#     )\n",
    "\n",
    "#     scheduler = DDPMScheduler(\n",
    "#         num_train_timesteps=num_train_timesteps,\n",
    "#         beta_start=beta_start,\n",
    "#         beta_end=beta_end,\n",
    "#         beta_schedule=beta_schedule,\n",
    "#     )\n",
    "\n",
    "    scheduler = DEISMultistepScheduler(\n",
    "        num_train_timesteps=num_train_timesteps,\n",
    "        beta_start=beta_start,\n",
    "        beta_end=beta_end,\n",
    "        beta_schedule=beta_schedule,\n",
    "    )\n",
    "\n",
    "    estimator = TimeGradEstimator(\n",
    "        freq=dataset[\"metadata\"].freq,\n",
    "        prediction_length=dataset[\"metadata\"].prediction_length,\n",
    "        input_size=int(dataset[\"metadata\"].feat_static_cat[0].cardinality),\n",
    "        scheduler=scheduler,\n",
    "        context_length=dataset[\"metadata\"].prediction_length * context_length_coef,\n",
    "        num_layers=num_layers,\n",
    "        hidden_size=hidden_size,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        dropout_rate=dropout_rate,\n",
    "        scaling=\"mean\",\n",
    "        lags_seq=lags_seq,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        batch_size=batch_size,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "        trainer_kwargs=dict(max_epochs=max_epochs, accelerator=\"gpu\", devices=\"1\"),\n",
    "    )\n",
    "\n",
    "    return estimator.train(dataset[\"train\"], cache_data=True, shuffle_buffer_length=1024)\n",
    "\n",
    "def prepare_metrics(dataset, predictor, num_samples=100):\n",
    "    evaluator = MultivariateEvaluator(\n",
    "        quantiles=(np.arange(20) / 20.0)[1:], target_agg_funcs={\"sum\": np.sum}\n",
    "    )\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset[\"test\"], predictor=predictor, num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    targets = list(ts_it)\n",
    "    agg_metric, _ = evaluator(targets, forecasts, num_series=len(dataset[\"test\"]))\n",
    "\n",
    "    return forecasts, targets, agg_metric\n",
    "\n",
    "def prepare_statistics(dataset, forecasts, targets, agg_metric, precision=3):\n",
    "    print(\"CRPS: {}\".format(round(agg_metric[\"mean_wQuantileLoss\"], precision)))\n",
    "    print(\"ND: {}\".format(round(agg_metric[\"ND\"], precision)))\n",
    "    print(\"NRMSE: {}\".format(round(agg_metric[\"NRMSE\"], precision)))\n",
    "    print(\"MSE: {}\".format(round(agg_metric[\"MSE\"], precision)))\n",
    "\n",
    "    print(\"-\" * 32)\n",
    "\n",
    "    print(\"CRPS-Sum: {}\".format(round(agg_metric[\"m_sum_mean_wQuantileLoss\"], precision)))\n",
    "    print(\"ND-Sum: {}\".format(round(agg_metric[\"m_sum_ND\"], precision)))\n",
    "    print(\"NRMSE-Sum: {}\".format(round(agg_metric[\"m_sum_NRMSE\"], precision)))\n",
    "    print(\"MSE-Sum: {}\".format(round(agg_metric[\"m_sum_MSE\"], precision)))\n",
    "\n",
    "    plot(\n",
    "        target=targets[0],\n",
    "        forecast=forecasts[0],\n",
    "        prediction_length=dataset[\"metadata\"].prediction_length,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use different types of schedulers\n",
    "# check different trainer_kwargs\n",
    "HYPERPARAMETERS = {\n",
    "    \"max_epochs\":            [128, 256, 512],\n",
    "    \"num_train_timesteps\":   [50, 100, 150, 200, 250],       # explore\n",
    "    \"beta_start\":            [1e-4],\n",
    "    \"beta_end\":              [0.1],\n",
    "    \"beta_schedule\":         [\"linear\"],                     # explore\n",
    "    \"context_length_coef\":   [1, 2, 3, 4, 5],                # explore\n",
    "    \"num_layers\":            [2, 3, 5],                      # explore\n",
    "    \"hidden_size\":           [32, 64, 128],                  # explore\n",
    "    \"lr\":                    [1e-5, 5e-5, 1e-4, 5e-4, 1e-3], # explore\n",
    "    \"weight_decay\":          [1e-9, 1e-8, 1e-7],\n",
    "    \"dropout_rate\":          [0.0, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"lags_seq\":              [None, [1]],                    # explore\n",
    "    \"num_inference_steps\":   [49, 99, 149, 199, 249],\n",
    "    \"batch_size\":            [32, 64, 128],\n",
    "    \"num_batches_per_epoch\": [32, 64, 128],\n",
    "    \"num_samples\":           [50, 100, 200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/time_feature/_base.py:243: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = to_offset(freq_str)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                             | Out sizes        \n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 186 K  | [[1, 1], [1, 1], [1, 72, 5], [1, 72, 137], [1, 72, 137], [1, 24, 5]] | [1, 100, 24, 137]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "186 K     Trainable params\n",
      "0         Non-trainable params\n",
      "186 K     Total params\n",
      "0.745     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c0d2006c4f426f96143559b9105e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41120 (best 0.41120), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.31175 (best 0.31175), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.14226 (best 0.14226), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.08811 (best 0.08811), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.07767 (best 0.07767), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.07093 (best 0.07093), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.06598 (best 0.06598), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.06143 (best 0.06143), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.05992 (best 0.05992), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.05841 (best 0.05841), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.05691 (best 0.05691), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.05547 (best 0.05547), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.05533 (best 0.05533), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.05484 (best 0.05484), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' reached 0.05418 (best 0.05418), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=14-step=960.ckpt' as top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.05330 (best 0.05330), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' reached 0.05304 (best 0.05304), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=16-step=1088.ckpt' as top 1\n",
      "Epoch 17, global step 1152: 'train_loss' reached 0.05207 (best 0.05207), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=17-step=1152.ckpt' as top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.05185 (best 0.05185), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' reached 0.05132 (best 0.05132), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=19-step=1280.ckpt' as top 1\n",
      "Epoch 20, global step 1344: 'train_loss' reached 0.05077 (best 0.05077), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=20-step=1344.ckpt' as top 1\n",
      "Epoch 21, global step 1408: 'train_loss' reached 0.05069 (best 0.05069), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=21-step=1408.ckpt' as top 1\n",
      "Epoch 22, global step 1472: 'train_loss' reached 0.05032 (best 0.05032), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=22-step=1472.ckpt' as top 1\n",
      "Epoch 23, global step 1536: 'train_loss' reached 0.04983 (best 0.04983), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=23-step=1536.ckpt' as top 1\n",
      "Epoch 24, global step 1600: 'train_loss' reached 0.04952 (best 0.04952), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=24-step=1600.ckpt' as top 1\n",
      "Epoch 25, global step 1664: 'train_loss' reached 0.04904 (best 0.04904), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=25-step=1664.ckpt' as top 1\n",
      "Epoch 26, global step 1728: 'train_loss' reached 0.04796 (best 0.04796), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=26-step=1728.ckpt' as top 1\n",
      "Epoch 27, global step 1792: 'train_loss' was not in top 1\n",
      "Epoch 28, global step 1856: 'train_loss' was not in top 1\n",
      "Epoch 29, global step 1920: 'train_loss' was not in top 1\n",
      "Epoch 30, global step 1984: 'train_loss' reached 0.04692 (best 0.04692), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=30-step=1984.ckpt' as top 1\n",
      "Epoch 31, global step 2048: 'train_loss' was not in top 1\n",
      "Epoch 32, global step 2112: 'train_loss' was not in top 1\n",
      "Epoch 33, global step 2176: 'train_loss' was not in top 1\n",
      "Epoch 34, global step 2240: 'train_loss' reached 0.04639 (best 0.04639), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=34-step=2240.ckpt' as top 1\n",
      "Epoch 35, global step 2304: 'train_loss' was not in top 1\n",
      "Epoch 36, global step 2368: 'train_loss' reached 0.04616 (best 0.04616), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=36-step=2368.ckpt' as top 1\n",
      "Epoch 37, global step 2432: 'train_loss' reached 0.04609 (best 0.04609), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=37-step=2432.ckpt' as top 1\n",
      "Epoch 38, global step 2496: 'train_loss' reached 0.04511 (best 0.04511), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=38-step=2496.ckpt' as top 1\n",
      "Epoch 39, global step 2560: 'train_loss' was not in top 1\n",
      "Epoch 40, global step 2624: 'train_loss' was not in top 1\n",
      "Epoch 41, global step 2688: 'train_loss' reached 0.04499 (best 0.04499), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=41-step=2688.ckpt' as top 1\n",
      "Epoch 42, global step 2752: 'train_loss' was not in top 1\n",
      "Epoch 43, global step 2816: 'train_loss' was not in top 1\n",
      "Epoch 44, global step 2880: 'train_loss' was not in top 1\n",
      "Epoch 45, global step 2944: 'train_loss' reached 0.04478 (best 0.04478), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=45-step=2944.ckpt' as top 1\n",
      "Epoch 46, global step 3008: 'train_loss' reached 0.04426 (best 0.04426), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=46-step=3008.ckpt' as top 1\n",
      "Epoch 47, global step 3072: 'train_loss' was not in top 1\n",
      "Epoch 48, global step 3136: 'train_loss' reached 0.04417 (best 0.04417), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=48-step=3136.ckpt' as top 1\n",
      "Epoch 49, global step 3200: 'train_loss' reached 0.04405 (best 0.04405), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=49-step=3200.ckpt' as top 1\n",
      "Epoch 50, global step 3264: 'train_loss' reached 0.04387 (best 0.04387), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=50-step=3264.ckpt' as top 1\n",
      "Epoch 51, global step 3328: 'train_loss' reached 0.04353 (best 0.04353), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=51-step=3328.ckpt' as top 1\n",
      "Epoch 52, global step 3392: 'train_loss' was not in top 1\n",
      "Epoch 53, global step 3456: 'train_loss' was not in top 1\n",
      "Epoch 54, global step 3520: 'train_loss' reached 0.04304 (best 0.04304), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=54-step=3520.ckpt' as top 1\n",
      "Epoch 55, global step 3584: 'train_loss' was not in top 1\n",
      "Epoch 56, global step 3648: 'train_loss' was not in top 1\n",
      "Epoch 57, global step 3712: 'train_loss' reached 0.04274 (best 0.04274), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=57-step=3712.ckpt' as top 1\n",
      "Epoch 58, global step 3776: 'train_loss' was not in top 1\n",
      "Epoch 59, global step 3840: 'train_loss' was not in top 1\n",
      "Epoch 60, global step 3904: 'train_loss' reached 0.04240 (best 0.04240), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=60-step=3904.ckpt' as top 1\n",
      "Epoch 61, global step 3968: 'train_loss' was not in top 1\n",
      "Epoch 62, global step 4032: 'train_loss' was not in top 1\n",
      "Epoch 63, global step 4096: 'train_loss' was not in top 1\n",
      "Epoch 64, global step 4160: 'train_loss' was not in top 1\n",
      "Epoch 65, global step 4224: 'train_loss' reached 0.04213 (best 0.04213), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=65-step=4224.ckpt' as top 1\n",
      "Epoch 66, global step 4288: 'train_loss' was not in top 1\n",
      "Epoch 67, global step 4352: 'train_loss' reached 0.04212 (best 0.04212), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=67-step=4352.ckpt' as top 1\n",
      "Epoch 68, global step 4416: 'train_loss' was not in top 1\n",
      "Epoch 69, global step 4480: 'train_loss' was not in top 1\n",
      "Epoch 70, global step 4544: 'train_loss' was not in top 1\n",
      "Epoch 71, global step 4608: 'train_loss' was not in top 1\n",
      "Epoch 72, global step 4672: 'train_loss' was not in top 1\n",
      "Epoch 73, global step 4736: 'train_loss' reached 0.04143 (best 0.04143), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=73-step=4736.ckpt' as top 1\n",
      "Epoch 74, global step 4800: 'train_loss' was not in top 1\n",
      "Epoch 75, global step 4864: 'train_loss' was not in top 1\n",
      "Epoch 76, global step 4928: 'train_loss' was not in top 1\n",
      "Epoch 77, global step 4992: 'train_loss' was not in top 1\n",
      "Epoch 78, global step 5056: 'train_loss' was not in top 1\n",
      "Epoch 79, global step 5120: 'train_loss' was not in top 1\n",
      "Epoch 80, global step 5184: 'train_loss' reached 0.04105 (best 0.04105), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=80-step=5184.ckpt' as top 1\n",
      "Epoch 81, global step 5248: 'train_loss' was not in top 1\n",
      "Epoch 82, global step 5312: 'train_loss' was not in top 1\n",
      "Epoch 83, global step 5376: 'train_loss' reached 0.04102 (best 0.04102), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=83-step=5376.ckpt' as top 1\n",
      "Epoch 84, global step 5440: 'train_loss' was not in top 1\n",
      "Epoch 85, global step 5504: 'train_loss' was not in top 1\n",
      "Epoch 86, global step 5568: 'train_loss' was not in top 1\n",
      "Epoch 87, global step 5632: 'train_loss' was not in top 1\n",
      "Epoch 88, global step 5696: 'train_loss' was not in top 1\n",
      "Epoch 89, global step 5760: 'train_loss' was not in top 1\n",
      "Epoch 90, global step 5824: 'train_loss' was not in top 1\n",
      "Epoch 91, global step 5888: 'train_loss' was not in top 1\n",
      "Epoch 92, global step 5952: 'train_loss' reached 0.04096 (best 0.04096), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=92-step=5952.ckpt' as top 1\n",
      "Epoch 93, global step 6016: 'train_loss' was not in top 1\n",
      "Epoch 94, global step 6080: 'train_loss' reached 0.04093 (best 0.04093), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=94-step=6080.ckpt' as top 1\n",
      "Epoch 95, global step 6144: 'train_loss' was not in top 1\n",
      "Epoch 96, global step 6208: 'train_loss' reached 0.04055 (best 0.04055), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=96-step=6208.ckpt' as top 1\n",
      "Epoch 97, global step 6272: 'train_loss' was not in top 1\n",
      "Epoch 98, global step 6336: 'train_loss' was not in top 1\n",
      "Epoch 99, global step 6400: 'train_loss' was not in top 1\n",
      "Epoch 100, global step 6464: 'train_loss' reached 0.04048 (best 0.04048), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=100-step=6464.ckpt' as top 1\n",
      "Epoch 101, global step 6528: 'train_loss' reached 0.03971 (best 0.03971), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=101-step=6528.ckpt' as top 1\n",
      "Epoch 102, global step 6592: 'train_loss' was not in top 1\n",
      "Epoch 103, global step 6656: 'train_loss' was not in top 1\n",
      "Epoch 104, global step 6720: 'train_loss' was not in top 1\n",
      "Epoch 105, global step 6784: 'train_loss' was not in top 1\n",
      "Epoch 106, global step 6848: 'train_loss' was not in top 1\n",
      "Epoch 107, global step 6912: 'train_loss' was not in top 1\n",
      "Epoch 108, global step 6976: 'train_loss' was not in top 1\n",
      "Epoch 109, global step 7040: 'train_loss' was not in top 1\n",
      "Epoch 110, global step 7104: 'train_loss' was not in top 1\n",
      "Epoch 111, global step 7168: 'train_loss' was not in top 1\n",
      "Epoch 112, global step 7232: 'train_loss' was not in top 1\n",
      "Epoch 113, global step 7296: 'train_loss' was not in top 1\n",
      "Epoch 114, global step 7360: 'train_loss' reached 0.03966 (best 0.03966), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=114-step=7360.ckpt' as top 1\n",
      "Epoch 115, global step 7424: 'train_loss' was not in top 1\n",
      "Epoch 116, global step 7488: 'train_loss' was not in top 1\n",
      "Epoch 117, global step 7552: 'train_loss' was not in top 1\n",
      "Epoch 118, global step 7616: 'train_loss' was not in top 1\n",
      "Epoch 119, global step 7680: 'train_loss' was not in top 1\n",
      "Epoch 120, global step 7744: 'train_loss' was not in top 1\n",
      "Epoch 121, global step 7808: 'train_loss' was not in top 1\n",
      "Epoch 122, global step 7872: 'train_loss' was not in top 1\n",
      "Epoch 123, global step 7936: 'train_loss' was not in top 1\n",
      "Epoch 124, global step 8000: 'train_loss' was not in top 1\n",
      "Epoch 125, global step 8064: 'train_loss' was not in top 1\n",
      "Epoch 126, global step 8128: 'train_loss' reached 0.03900 (best 0.03900), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=126-step=8128.ckpt' as top 1\n",
      "Epoch 127, global step 8192: 'train_loss' was not in top 1\n",
      "Epoch 128, global step 8256: 'train_loss' was not in top 1\n",
      "Epoch 129, global step 8320: 'train_loss' was not in top 1\n",
      "Epoch 130, global step 8384: 'train_loss' was not in top 1\n",
      "Epoch 131, global step 8448: 'train_loss' was not in top 1\n",
      "Epoch 132, global step 8512: 'train_loss' was not in top 1\n",
      "Epoch 133, global step 8576: 'train_loss' was not in top 1\n",
      "Epoch 134, global step 8640: 'train_loss' was not in top 1\n",
      "Epoch 135, global step 8704: 'train_loss' was not in top 1\n",
      "Epoch 136, global step 8768: 'train_loss' was not in top 1\n",
      "Epoch 137, global step 8832: 'train_loss' was not in top 1\n",
      "Epoch 138, global step 8896: 'train_loss' was not in top 1\n",
      "Epoch 139, global step 8960: 'train_loss' was not in top 1\n",
      "Epoch 140, global step 9024: 'train_loss' was not in top 1\n",
      "Epoch 141, global step 9088: 'train_loss' was not in top 1\n",
      "Epoch 142, global step 9152: 'train_loss' was not in top 1\n",
      "Epoch 143, global step 9216: 'train_loss' was not in top 1\n",
      "Epoch 144, global step 9280: 'train_loss' was not in top 1\n",
      "Epoch 145, global step 9344: 'train_loss' was not in top 1\n",
      "Epoch 146, global step 9408: 'train_loss' was not in top 1\n",
      "Epoch 147, global step 9472: 'train_loss' was not in top 1\n",
      "Epoch 148, global step 9536: 'train_loss' was not in top 1\n",
      "Epoch 149, global step 9600: 'train_loss' was not in top 1\n",
      "Epoch 150, global step 9664: 'train_loss' was not in top 1\n",
      "Epoch 151, global step 9728: 'train_loss' was not in top 1\n",
      "Epoch 152, global step 9792: 'train_loss' was not in top 1\n",
      "Epoch 153, global step 9856: 'train_loss' was not in top 1\n",
      "Epoch 154, global step 9920: 'train_loss' was not in top 1\n",
      "Epoch 155, global step 9984: 'train_loss' was not in top 1\n",
      "Epoch 156, global step 10048: 'train_loss' was not in top 1\n",
      "Epoch 157, global step 10112: 'train_loss' was not in top 1\n",
      "Epoch 158, global step 10176: 'train_loss' was not in top 1\n",
      "Epoch 159, global step 10240: 'train_loss' was not in top 1\n",
      "Epoch 160, global step 10304: 'train_loss' was not in top 1\n",
      "Epoch 161, global step 10368: 'train_loss' was not in top 1\n",
      "Epoch 162, global step 10432: 'train_loss' was not in top 1\n",
      "Epoch 163, global step 10496: 'train_loss' was not in top 1\n",
      "Epoch 164, global step 10560: 'train_loss' was not in top 1\n",
      "Epoch 165, global step 10624: 'train_loss' was not in top 1\n",
      "Epoch 166, global step 10688: 'train_loss' was not in top 1\n",
      "Epoch 167, global step 10752: 'train_loss' was not in top 1\n",
      "Epoch 168, global step 10816: 'train_loss' was not in top 1\n",
      "Epoch 169, global step 10880: 'train_loss' was not in top 1\n",
      "Epoch 170, global step 10944: 'train_loss' was not in top 1\n",
      "Epoch 171, global step 11008: 'train_loss' was not in top 1\n",
      "Epoch 172, global step 11072: 'train_loss' was not in top 1\n",
      "Epoch 173, global step 11136: 'train_loss' was not in top 1\n",
      "Epoch 174, global step 11200: 'train_loss' was not in top 1\n",
      "Epoch 175, global step 11264: 'train_loss' was not in top 1\n",
      "Epoch 176, global step 11328: 'train_loss' was not in top 1\n",
      "Epoch 177, global step 11392: 'train_loss' was not in top 1\n",
      "Epoch 178, global step 11456: 'train_loss' was not in top 1\n",
      "Epoch 179, global step 11520: 'train_loss' was not in top 1\n",
      "Epoch 180, global step 11584: 'train_loss' was not in top 1\n",
      "Epoch 181, global step 11648: 'train_loss' was not in top 1\n",
      "Epoch 182, global step 11712: 'train_loss' was not in top 1\n",
      "Epoch 183, global step 11776: 'train_loss' was not in top 1\n",
      "Epoch 184, global step 11840: 'train_loss' was not in top 1\n",
      "Epoch 185, global step 11904: 'train_loss' was not in top 1\n",
      "Epoch 186, global step 11968: 'train_loss' was not in top 1\n",
      "Epoch 187, global step 12032: 'train_loss' was not in top 1\n",
      "Epoch 188, global step 12096: 'train_loss' was not in top 1\n",
      "Epoch 189, global step 12160: 'train_loss' was not in top 1\n",
      "Epoch 190, global step 12224: 'train_loss' was not in top 1\n",
      "Epoch 191, global step 12288: 'train_loss' was not in top 1\n",
      "Epoch 192, global step 12352: 'train_loss' was not in top 1\n",
      "Epoch 193, global step 12416: 'train_loss' was not in top 1\n",
      "Epoch 194, global step 12480: 'train_loss' was not in top 1\n",
      "Epoch 195, global step 12544: 'train_loss' was not in top 1\n",
      "Epoch 196, global step 12608: 'train_loss' was not in top 1\n",
      "Epoch 197, global step 12672: 'train_loss' was not in top 1\n",
      "Epoch 198, global step 12736: 'train_loss' was not in top 1\n",
      "Epoch 199, global step 12800: 'train_loss' was not in top 1\n",
      "Epoch 200, global step 12864: 'train_loss' was not in top 1\n",
      "Epoch 201, global step 12928: 'train_loss' was not in top 1\n",
      "Epoch 202, global step 12992: 'train_loss' was not in top 1\n",
      "Epoch 203, global step 13056: 'train_loss' was not in top 1\n",
      "Epoch 204, global step 13120: 'train_loss' was not in top 1\n",
      "Epoch 205, global step 13184: 'train_loss' was not in top 1\n",
      "Epoch 206, global step 13248: 'train_loss' was not in top 1\n",
      "Epoch 207, global step 13312: 'train_loss' was not in top 1\n",
      "Epoch 208, global step 13376: 'train_loss' was not in top 1\n",
      "Epoch 209, global step 13440: 'train_loss' reached 0.03887 (best 0.03887), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_1/checkpoints/epoch=209-step=13440.ckpt' as top 1\n",
      "Epoch 210, global step 13504: 'train_loss' was not in top 1\n",
      "Epoch 211, global step 13568: 'train_loss' was not in top 1\n",
      "Epoch 212, global step 13632: 'train_loss' was not in top 1\n",
      "Epoch 213, global step 13696: 'train_loss' was not in top 1\n",
      "Epoch 214, global step 13760: 'train_loss' was not in top 1\n",
      "Epoch 215, global step 13824: 'train_loss' was not in top 1\n",
      "Epoch 216, global step 13888: 'train_loss' was not in top 1\n",
      "Epoch 217, global step 13952: 'train_loss' was not in top 1\n",
      "Epoch 218, global step 14016: 'train_loss' was not in top 1\n",
      "Epoch 219, global step 14080: 'train_loss' was not in top 1\n",
      "Epoch 220, global step 14144: 'train_loss' was not in top 1\n",
      "Epoch 221, global step 14208: 'train_loss' was not in top 1\n",
      "Epoch 222, global step 14272: 'train_loss' was not in top 1\n",
      "Epoch 223, global step 14336: 'train_loss' was not in top 1\n",
      "Epoch 224, global step 14400: 'train_loss' was not in top 1\n",
      "Epoch 225, global step 14464: 'train_loss' was not in top 1\n",
      "Epoch 226, global step 14528: 'train_loss' was not in top 1\n",
      "Epoch 227, global step 14592: 'train_loss' was not in top 1\n",
      "Epoch 228, global step 14656: 'train_loss' was not in top 1\n",
      "Epoch 229, global step 14720: 'train_loss' was not in top 1\n",
      "Epoch 230, global step 14784: 'train_loss' was not in top 1\n",
      "Epoch 231, global step 14848: 'train_loss' was not in top 1\n",
      "Epoch 232, global step 14912: 'train_loss' was not in top 1\n",
      "Epoch 233, global step 14976: 'train_loss' was not in top 1\n",
      "Epoch 234, global step 15040: 'train_loss' was not in top 1\n",
      "Epoch 235, global step 15104: 'train_loss' was not in top 1\n",
      "Epoch 236, global step 15168: 'train_loss' was not in top 1\n",
      "Epoch 237, global step 15232: 'train_loss' was not in top 1\n",
      "Epoch 238, global step 15296: 'train_loss' was not in top 1\n",
      "Epoch 239, global step 15360: 'train_loss' was not in top 1\n",
      "Epoch 240, global step 15424: 'train_loss' was not in top 1\n",
      "Epoch 241, global step 15488: 'train_loss' was not in top 1\n",
      "Epoch 242, global step 15552: 'train_loss' was not in top 1\n",
      "Epoch 243, global step 15616: 'train_loss' was not in top 1\n",
      "Epoch 244, global step 15680: 'train_loss' was not in top 1\n",
      "Epoch 245, global step 15744: 'train_loss' was not in top 1\n",
      "Epoch 246, global step 15808: 'train_loss' was not in top 1\n",
      "Epoch 247, global step 15872: 'train_loss' was not in top 1\n",
      "Epoch 248, global step 15936: 'train_loss' was not in top 1\n",
      "Epoch 249, global step 16000: 'train_loss' was not in top 1\n",
      "Epoch 250, global step 16064: 'train_loss' was not in top 1\n",
      "Epoch 251, global step 16128: 'train_loss' was not in top 1\n",
      "Epoch 252, global step 16192: 'train_loss' was not in top 1\n",
      "Epoch 253, global step 16256: 'train_loss' was not in top 1\n",
      "Epoch 254, global step 16320: 'train_loss' was not in top 1\n",
      "Epoch 255, global step 16384: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=256` reached.\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 247.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 195.84it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.84it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 221.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 271.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 228.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 253.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.94it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 214.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 250.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 251.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 262.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.36it/s]\n",
      "\n",
      "Running evaluation: 0it [00:00, ?it/s]\u001b[A/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "Running evaluation: 7it [00:00, 162.57it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/common.py:262: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  return pd.Period(val, freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/time_feature/_base.py:243: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  offset = to_offset(freq_str)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                             | Out sizes        \n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 432 K  | [[1, 1], [1, 1], [1, 72, 5], [1, 72, 370], [1, 72, 370], [1, 24, 5]] | [1, 100, 24, 370]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------\n",
      "432 K     Trainable params\n",
      "0         Non-trainable params\n",
      "432 K     Total params\n",
      "1.729     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f286588046ec4d25bd3e71198a3a4013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41410 (best 0.41410), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.30522 (best 0.30522), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.11737 (best 0.11737), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.07334 (best 0.07334), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.06130 (best 0.06130), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.05290 (best 0.05290), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.04940 (best 0.04940), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.04809 (best 0.04809), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.04619 (best 0.04619), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.04516 (best 0.04516), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.04366 (best 0.04366), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.04333 (best 0.04333), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.04259 (best 0.04259), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.04198 (best 0.04198), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' reached 0.04136 (best 0.04136), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=14-step=960.ckpt' as top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.04046 (best 0.04046), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' was not in top 1\n",
      "Epoch 17, global step 1152: 'train_loss' reached 0.03960 (best 0.03960), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=17-step=1152.ckpt' as top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.03911 (best 0.03911), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' reached 0.03910 (best 0.03910), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=19-step=1280.ckpt' as top 1\n",
      "Epoch 20, global step 1344: 'train_loss' reached 0.03806 (best 0.03806), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=20-step=1344.ckpt' as top 1\n",
      "Epoch 21, global step 1408: 'train_loss' was not in top 1\n",
      "Epoch 22, global step 1472: 'train_loss' was not in top 1\n",
      "Epoch 23, global step 1536: 'train_loss' reached 0.03762 (best 0.03762), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=23-step=1536.ckpt' as top 1\n",
      "Epoch 24, global step 1600: 'train_loss' reached 0.03760 (best 0.03760), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=24-step=1600.ckpt' as top 1\n",
      "Epoch 25, global step 1664: 'train_loss' reached 0.03748 (best 0.03748), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=25-step=1664.ckpt' as top 1\n",
      "Epoch 26, global step 1728: 'train_loss' was not in top 1\n",
      "Epoch 27, global step 1792: 'train_loss' reached 0.03686 (best 0.03686), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=27-step=1792.ckpt' as top 1\n",
      "Epoch 28, global step 1856: 'train_loss' reached 0.03670 (best 0.03670), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=28-step=1856.ckpt' as top 1\n",
      "Epoch 29, global step 1920: 'train_loss' reached 0.03661 (best 0.03661), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=29-step=1920.ckpt' as top 1\n",
      "Epoch 30, global step 1984: 'train_loss' reached 0.03654 (best 0.03654), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=30-step=1984.ckpt' as top 1\n",
      "Epoch 31, global step 2048: 'train_loss' reached 0.03557 (best 0.03557), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=31-step=2048.ckpt' as top 1\n",
      "Epoch 32, global step 2112: 'train_loss' was not in top 1\n",
      "Epoch 33, global step 2176: 'train_loss' was not in top 1\n",
      "Epoch 34, global step 2240: 'train_loss' reached 0.03520 (best 0.03520), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=34-step=2240.ckpt' as top 1\n",
      "Epoch 35, global step 2304: 'train_loss' was not in top 1\n",
      "Epoch 36, global step 2368: 'train_loss' was not in top 1\n",
      "Epoch 37, global step 2432: 'train_loss' was not in top 1\n",
      "Epoch 38, global step 2496: 'train_loss' reached 0.03463 (best 0.03463), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=38-step=2496.ckpt' as top 1\n",
      "Epoch 39, global step 2560: 'train_loss' was not in top 1\n",
      "Epoch 40, global step 2624: 'train_loss' reached 0.03457 (best 0.03457), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=40-step=2624.ckpt' as top 1\n",
      "Epoch 41, global step 2688: 'train_loss' was not in top 1\n",
      "Epoch 42, global step 2752: 'train_loss' reached 0.03402 (best 0.03402), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=42-step=2752.ckpt' as top 1\n",
      "Epoch 43, global step 2816: 'train_loss' reached 0.03392 (best 0.03392), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=43-step=2816.ckpt' as top 1\n",
      "Epoch 44, global step 2880: 'train_loss' was not in top 1\n",
      "Epoch 45, global step 2944: 'train_loss' reached 0.03387 (best 0.03387), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=45-step=2944.ckpt' as top 1\n",
      "Epoch 46, global step 3008: 'train_loss' reached 0.03373 (best 0.03373), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=46-step=3008.ckpt' as top 1\n",
      "Epoch 47, global step 3072: 'train_loss' was not in top 1\n",
      "Epoch 48, global step 3136: 'train_loss' was not in top 1\n",
      "Epoch 49, global step 3200: 'train_loss' reached 0.03358 (best 0.03358), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=49-step=3200.ckpt' as top 1\n",
      "Epoch 50, global step 3264: 'train_loss' was not in top 1\n",
      "Epoch 51, global step 3328: 'train_loss' was not in top 1\n",
      "Epoch 52, global step 3392: 'train_loss' reached 0.03343 (best 0.03343), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=52-step=3392.ckpt' as top 1\n",
      "Epoch 53, global step 3456: 'train_loss' reached 0.03321 (best 0.03321), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=53-step=3456.ckpt' as top 1\n",
      "Epoch 54, global step 3520: 'train_loss' was not in top 1\n",
      "Epoch 55, global step 3584: 'train_loss' reached 0.03284 (best 0.03284), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=55-step=3584.ckpt' as top 1\n",
      "Epoch 56, global step 3648: 'train_loss' was not in top 1\n",
      "Epoch 57, global step 3712: 'train_loss' was not in top 1\n",
      "Epoch 58, global step 3776: 'train_loss' was not in top 1\n",
      "Epoch 59, global step 3840: 'train_loss' was not in top 1\n",
      "Epoch 60, global step 3904: 'train_loss' reached 0.03235 (best 0.03235), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=60-step=3904.ckpt' as top 1\n",
      "Epoch 61, global step 3968: 'train_loss' was not in top 1\n",
      "Epoch 62, global step 4032: 'train_loss' was not in top 1\n",
      "Epoch 63, global step 4096: 'train_loss' was not in top 1\n",
      "Epoch 64, global step 4160: 'train_loss' was not in top 1\n",
      "Epoch 65, global step 4224: 'train_loss' reached 0.03180 (best 0.03180), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=65-step=4224.ckpt' as top 1\n",
      "Epoch 66, global step 4288: 'train_loss' was not in top 1\n",
      "Epoch 67, global step 4352: 'train_loss' was not in top 1\n",
      "Epoch 68, global step 4416: 'train_loss' reached 0.03140 (best 0.03140), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=68-step=4416.ckpt' as top 1\n",
      "Epoch 69, global step 4480: 'train_loss' was not in top 1\n",
      "Epoch 70, global step 4544: 'train_loss' was not in top 1\n",
      "Epoch 71, global step 4608: 'train_loss' was not in top 1\n",
      "Epoch 72, global step 4672: 'train_loss' was not in top 1\n",
      "Epoch 73, global step 4736: 'train_loss' was not in top 1\n",
      "Epoch 74, global step 4800: 'train_loss' was not in top 1\n",
      "Epoch 75, global step 4864: 'train_loss' was not in top 1\n",
      "Epoch 76, global step 4928: 'train_loss' was not in top 1\n",
      "Epoch 77, global step 4992: 'train_loss' reached 0.03118 (best 0.03118), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=77-step=4992.ckpt' as top 1\n",
      "Epoch 78, global step 5056: 'train_loss' was not in top 1\n",
      "Epoch 79, global step 5120: 'train_loss' was not in top 1\n",
      "Epoch 80, global step 5184: 'train_loss' was not in top 1\n",
      "Epoch 81, global step 5248: 'train_loss' was not in top 1\n",
      "Epoch 82, global step 5312: 'train_loss' was not in top 1\n",
      "Epoch 83, global step 5376: 'train_loss' reached 0.03100 (best 0.03100), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=83-step=5376.ckpt' as top 1\n",
      "Epoch 84, global step 5440: 'train_loss' was not in top 1\n",
      "Epoch 85, global step 5504: 'train_loss' was not in top 1\n",
      "Epoch 86, global step 5568: 'train_loss' was not in top 1\n",
      "Epoch 87, global step 5632: 'train_loss' reached 0.03077 (best 0.03077), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=87-step=5632.ckpt' as top 1\n",
      "Epoch 88, global step 5696: 'train_loss' was not in top 1\n",
      "Epoch 89, global step 5760: 'train_loss' reached 0.03021 (best 0.03021), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=89-step=5760.ckpt' as top 1\n",
      "Epoch 90, global step 5824: 'train_loss' was not in top 1\n",
      "Epoch 91, global step 5888: 'train_loss' was not in top 1\n",
      "Epoch 92, global step 5952: 'train_loss' was not in top 1\n",
      "Epoch 93, global step 6016: 'train_loss' was not in top 1\n",
      "Epoch 94, global step 6080: 'train_loss' was not in top 1\n",
      "Epoch 95, global step 6144: 'train_loss' was not in top 1\n",
      "Epoch 96, global step 6208: 'train_loss' was not in top 1\n",
      "Epoch 97, global step 6272: 'train_loss' was not in top 1\n",
      "Epoch 98, global step 6336: 'train_loss' was not in top 1\n",
      "Epoch 99, global step 6400: 'train_loss' was not in top 1\n",
      "Epoch 100, global step 6464: 'train_loss' was not in top 1\n",
      "Epoch 101, global step 6528: 'train_loss' reached 0.02993 (best 0.02993), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=101-step=6528.ckpt' as top 1\n",
      "Epoch 102, global step 6592: 'train_loss' was not in top 1\n",
      "Epoch 103, global step 6656: 'train_loss' was not in top 1\n",
      "Epoch 104, global step 6720: 'train_loss' was not in top 1\n",
      "Epoch 105, global step 6784: 'train_loss' was not in top 1\n",
      "Epoch 106, global step 6848: 'train_loss' reached 0.02981 (best 0.02981), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=106-step=6848.ckpt' as top 1\n",
      "Epoch 107, global step 6912: 'train_loss' reached 0.02974 (best 0.02974), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=107-step=6912.ckpt' as top 1\n",
      "Epoch 108, global step 6976: 'train_loss' was not in top 1\n",
      "Epoch 109, global step 7040: 'train_loss' reached 0.02956 (best 0.02956), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=109-step=7040.ckpt' as top 1\n",
      "Epoch 110, global step 7104: 'train_loss' was not in top 1\n",
      "Epoch 111, global step 7168: 'train_loss' was not in top 1\n",
      "Epoch 112, global step 7232: 'train_loss' was not in top 1\n",
      "Epoch 113, global step 7296: 'train_loss' was not in top 1\n",
      "Epoch 114, global step 7360: 'train_loss' reached 0.02955 (best 0.02955), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=114-step=7360.ckpt' as top 1\n",
      "Epoch 115, global step 7424: 'train_loss' was not in top 1\n",
      "Epoch 116, global step 7488: 'train_loss' was not in top 1\n",
      "Epoch 117, global step 7552: 'train_loss' was not in top 1\n",
      "Epoch 118, global step 7616: 'train_loss' was not in top 1\n",
      "Epoch 119, global step 7680: 'train_loss' was not in top 1\n",
      "Epoch 120, global step 7744: 'train_loss' reached 0.02950 (best 0.02950), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=120-step=7744.ckpt' as top 1\n",
      "Epoch 121, global step 7808: 'train_loss' reached 0.02947 (best 0.02947), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=121-step=7808.ckpt' as top 1\n",
      "Epoch 122, global step 7872: 'train_loss' reached 0.02931 (best 0.02931), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=122-step=7872.ckpt' as top 1\n",
      "Epoch 123, global step 7936: 'train_loss' was not in top 1\n",
      "Epoch 124, global step 8000: 'train_loss' was not in top 1\n",
      "Epoch 125, global step 8064: 'train_loss' was not in top 1\n",
      "Epoch 126, global step 8128: 'train_loss' was not in top 1\n",
      "Epoch 127, global step 8192: 'train_loss' was not in top 1\n",
      "Epoch 128, global step 8256: 'train_loss' was not in top 1\n",
      "Epoch 129, global step 8320: 'train_loss' was not in top 1\n",
      "Epoch 130, global step 8384: 'train_loss' was not in top 1\n",
      "Epoch 131, global step 8448: 'train_loss' reached 0.02909 (best 0.02909), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=131-step=8448.ckpt' as top 1\n",
      "Epoch 132, global step 8512: 'train_loss' was not in top 1\n",
      "Epoch 133, global step 8576: 'train_loss' was not in top 1\n",
      "Epoch 134, global step 8640: 'train_loss' was not in top 1\n",
      "Epoch 135, global step 8704: 'train_loss' was not in top 1\n",
      "Epoch 136, global step 8768: 'train_loss' was not in top 1\n",
      "Epoch 137, global step 8832: 'train_loss' was not in top 1\n",
      "Epoch 138, global step 8896: 'train_loss' was not in top 1\n",
      "Epoch 139, global step 8960: 'train_loss' was not in top 1\n",
      "Epoch 140, global step 9024: 'train_loss' was not in top 1\n",
      "Epoch 141, global step 9088: 'train_loss' was not in top 1\n",
      "Epoch 142, global step 9152: 'train_loss' was not in top 1\n",
      "Epoch 143, global step 9216: 'train_loss' was not in top 1\n",
      "Epoch 144, global step 9280: 'train_loss' reached 0.02892 (best 0.02892), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=144-step=9280.ckpt' as top 1\n",
      "Epoch 145, global step 9344: 'train_loss' was not in top 1\n",
      "Epoch 146, global step 9408: 'train_loss' reached 0.02847 (best 0.02847), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=146-step=9408.ckpt' as top 1\n",
      "Epoch 147, global step 9472: 'train_loss' was not in top 1\n",
      "Epoch 148, global step 9536: 'train_loss' was not in top 1\n",
      "Epoch 149, global step 9600: 'train_loss' was not in top 1\n",
      "Epoch 150, global step 9664: 'train_loss' was not in top 1\n",
      "Epoch 151, global step 9728: 'train_loss' was not in top 1\n",
      "Epoch 152, global step 9792: 'train_loss' was not in top 1\n",
      "Epoch 153, global step 9856: 'train_loss' was not in top 1\n",
      "Epoch 154, global step 9920: 'train_loss' was not in top 1\n",
      "Epoch 155, global step 9984: 'train_loss' was not in top 1\n",
      "Epoch 156, global step 10048: 'train_loss' was not in top 1\n",
      "Epoch 157, global step 10112: 'train_loss' was not in top 1\n",
      "Epoch 158, global step 10176: 'train_loss' was not in top 1\n",
      "Epoch 159, global step 10240: 'train_loss' reached 0.02831 (best 0.02831), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=159-step=10240.ckpt' as top 1\n",
      "Epoch 160, global step 10304: 'train_loss' was not in top 1\n",
      "Epoch 161, global step 10368: 'train_loss' was not in top 1\n",
      "Epoch 162, global step 10432: 'train_loss' was not in top 1\n",
      "Epoch 163, global step 10496: 'train_loss' was not in top 1\n",
      "Epoch 164, global step 10560: 'train_loss' was not in top 1\n",
      "Epoch 165, global step 10624: 'train_loss' was not in top 1\n",
      "Epoch 166, global step 10688: 'train_loss' was not in top 1\n",
      "Epoch 167, global step 10752: 'train_loss' was not in top 1\n",
      "Epoch 168, global step 10816: 'train_loss' was not in top 1\n",
      "Epoch 169, global step 10880: 'train_loss' was not in top 1\n",
      "Epoch 170, global step 10944: 'train_loss' was not in top 1\n",
      "Epoch 171, global step 11008: 'train_loss' was not in top 1\n",
      "Epoch 172, global step 11072: 'train_loss' was not in top 1\n",
      "Epoch 173, global step 11136: 'train_loss' was not in top 1\n",
      "Epoch 174, global step 11200: 'train_loss' was not in top 1\n",
      "Epoch 175, global step 11264: 'train_loss' was not in top 1\n",
      "Epoch 176, global step 11328: 'train_loss' was not in top 1\n",
      "Epoch 177, global step 11392: 'train_loss' was not in top 1\n",
      "Epoch 178, global step 11456: 'train_loss' was not in top 1\n",
      "Epoch 179, global step 11520: 'train_loss' was not in top 1\n",
      "Epoch 180, global step 11584: 'train_loss' was not in top 1\n",
      "Epoch 181, global step 11648: 'train_loss' was not in top 1\n",
      "Epoch 182, global step 11712: 'train_loss' was not in top 1\n",
      "Epoch 183, global step 11776: 'train_loss' was not in top 1\n",
      "Epoch 184, global step 11840: 'train_loss' was not in top 1\n",
      "Epoch 185, global step 11904: 'train_loss' was not in top 1\n",
      "Epoch 186, global step 11968: 'train_loss' was not in top 1\n",
      "Epoch 187, global step 12032: 'train_loss' was not in top 1\n",
      "Epoch 188, global step 12096: 'train_loss' was not in top 1\n",
      "Epoch 189, global step 12160: 'train_loss' was not in top 1\n",
      "Epoch 190, global step 12224: 'train_loss' was not in top 1\n",
      "Epoch 191, global step 12288: 'train_loss' was not in top 1\n",
      "Epoch 192, global step 12352: 'train_loss' was not in top 1\n",
      "Epoch 193, global step 12416: 'train_loss' was not in top 1\n",
      "Epoch 194, global step 12480: 'train_loss' was not in top 1\n",
      "Epoch 195, global step 12544: 'train_loss' was not in top 1\n",
      "Epoch 196, global step 12608: 'train_loss' was not in top 1\n",
      "Epoch 197, global step 12672: 'train_loss' was not in top 1\n",
      "Epoch 198, global step 12736: 'train_loss' was not in top 1\n",
      "Epoch 199, global step 12800: 'train_loss' was not in top 1\n",
      "Epoch 200, global step 12864: 'train_loss' was not in top 1\n",
      "Epoch 201, global step 12928: 'train_loss' was not in top 1\n",
      "Epoch 202, global step 12992: 'train_loss' was not in top 1\n",
      "Epoch 203, global step 13056: 'train_loss' was not in top 1\n",
      "Epoch 204, global step 13120: 'train_loss' was not in top 1\n",
      "Epoch 205, global step 13184: 'train_loss' was not in top 1\n",
      "Epoch 206, global step 13248: 'train_loss' was not in top 1\n",
      "Epoch 207, global step 13312: 'train_loss' was not in top 1\n",
      "Epoch 208, global step 13376: 'train_loss' was not in top 1\n",
      "Epoch 209, global step 13440: 'train_loss' was not in top 1\n",
      "Epoch 210, global step 13504: 'train_loss' was not in top 1\n",
      "Epoch 211, global step 13568: 'train_loss' was not in top 1\n",
      "Epoch 212, global step 13632: 'train_loss' was not in top 1\n",
      "Epoch 213, global step 13696: 'train_loss' was not in top 1\n",
      "Epoch 214, global step 13760: 'train_loss' reached 0.02822 (best 0.02822), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=214-step=13760.ckpt' as top 1\n",
      "Epoch 215, global step 13824: 'train_loss' was not in top 1\n",
      "Epoch 216, global step 13888: 'train_loss' was not in top 1\n",
      "Epoch 217, global step 13952: 'train_loss' was not in top 1\n",
      "Epoch 218, global step 14016: 'train_loss' was not in top 1\n",
      "Epoch 219, global step 14080: 'train_loss' was not in top 1\n",
      "Epoch 220, global step 14144: 'train_loss' was not in top 1\n",
      "Epoch 221, global step 14208: 'train_loss' was not in top 1\n",
      "Epoch 222, global step 14272: 'train_loss' was not in top 1\n",
      "Epoch 223, global step 14336: 'train_loss' was not in top 1\n",
      "Epoch 224, global step 14400: 'train_loss' was not in top 1\n",
      "Epoch 225, global step 14464: 'train_loss' was not in top 1\n",
      "Epoch 226, global step 14528: 'train_loss' was not in top 1\n",
      "Epoch 227, global step 14592: 'train_loss' was not in top 1\n",
      "Epoch 228, global step 14656: 'train_loss' was not in top 1\n",
      "Epoch 229, global step 14720: 'train_loss' was not in top 1\n",
      "Epoch 230, global step 14784: 'train_loss' was not in top 1\n",
      "Epoch 231, global step 14848: 'train_loss' was not in top 1\n",
      "Epoch 232, global step 14912: 'train_loss' was not in top 1\n",
      "Epoch 233, global step 14976: 'train_loss' was not in top 1\n",
      "Epoch 234, global step 15040: 'train_loss' was not in top 1\n",
      "Epoch 235, global step 15104: 'train_loss' reached 0.02794 (best 0.02794), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_2/checkpoints/epoch=235-step=15104.ckpt' as top 1\n",
      "Epoch 236, global step 15168: 'train_loss' was not in top 1\n",
      "Epoch 237, global step 15232: 'train_loss' was not in top 1\n",
      "Epoch 238, global step 15296: 'train_loss' was not in top 1\n",
      "Epoch 239, global step 15360: 'train_loss' was not in top 1\n",
      "Epoch 240, global step 15424: 'train_loss' was not in top 1\n",
      "Epoch 241, global step 15488: 'train_loss' was not in top 1\n",
      "Epoch 242, global step 15552: 'train_loss' was not in top 1\n",
      "Epoch 243, global step 15616: 'train_loss' was not in top 1\n",
      "Epoch 244, global step 15680: 'train_loss' was not in top 1\n",
      "Epoch 245, global step 15744: 'train_loss' was not in top 1\n",
      "Epoch 246, global step 15808: 'train_loss' was not in top 1\n",
      "Epoch 247, global step 15872: 'train_loss' was not in top 1\n",
      "Epoch 248, global step 15936: 'train_loss' was not in top 1\n",
      "Epoch 249, global step 16000: 'train_loss' was not in top 1\n",
      "Epoch 250, global step 16064: 'train_loss' was not in top 1\n",
      "Epoch 251, global step 16128: 'train_loss' was not in top 1\n",
      "Epoch 252, global step 16192: 'train_loss' was not in top 1\n",
      "Epoch 253, global step 16256: 'train_loss' was not in top 1\n",
      "Epoch 254, global step 16320: 'train_loss' was not in top 1\n",
      "Epoch 255, global step 16384: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=256` reached.\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 209.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 242.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.97it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 296.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 207.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 208.61it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.23it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 198.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 227.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 253.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 208.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.97it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 258.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 266.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.40it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 204.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 299.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.35it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.00it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 274.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 200.77it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.18it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 254.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 220.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 217.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 114.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 226.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.32it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.07it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.37it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 220.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.06it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 214.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.77it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 152.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 44.69it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.56it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.92it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.90it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 219.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.78it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.72it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 250.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.65it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 273.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.43it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 249.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.94it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 211.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 251.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.83it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 272.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.52it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 282.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 212.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 215.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 221.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 269.80it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.24it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.34it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.03it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.26it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 298.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 275.33it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.73it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 265.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.45it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.64it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.09it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 293.82it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 248.11it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.54it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 276.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.49it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.01it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.43it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 268.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.67it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 256.79it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.23it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 278.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 279.95it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.38it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 247.88it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.53it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.58it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 261.60it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 254.44it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.46it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.41it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 246.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 255.31it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 266.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.87it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 281.21it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.86it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.93it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 284.29it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.25it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.36it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 294.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 199.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 206.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.74it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 203.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.99it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 202.71it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 210.68it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133: UserWarning: Warning: converting a masked element to nan.\n",
      "  return arr.astype(dtype, copy=True)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:531: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[\"NRMSE\"] = totals[\"RMSE\"] / totals[\"abs_target_mean\"]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:532: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[\"ND\"] = totals[\"abs_error\"] / totals[\"abs_target_sum\"]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:536: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  totals[f\"QuantileLoss[{quantile}]\"] / totals[\"abs_target_sum\"]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 205.47it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.89it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 291.76it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 178.17it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 222.66it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.57it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.50it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 216.42it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 270.19it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.02it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 296.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 259.39it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.13it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.70it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 199.51it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.14it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 289.04it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.10it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 286.63it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 249.62it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 290.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 201.55it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.22it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 295.15it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 198.30it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 262.85it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.08it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 213.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 277.75it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 283.28it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 288.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 263.96it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 280.24it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 297.59it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 242.16it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.68it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 292.48it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 218.12it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 287.91it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 272.27it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.98it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 196.05it/s]\n",
      "\n",
      "Running evaluation: 7it [00:00, 285.71it/s]\n",
      "\n",
      "Running evaluation: 0it [00:00, ?it/s]\u001b[A/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/evaluation/_base.py:672: FutureWarning: The provided callable <function sum at 0x110dbff60> is currently using DataFrame.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n",
      "  yield i.agg(agg_fun, axis=1)\n",
      "Running evaluation: 7it [00:00, 165.74it/s]\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/common.py:262: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  return pd.Period(val, freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:113: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  timestamp + len(data[FieldName.TARGET]) - 1,\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:242: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  index=pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:242: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  index=pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:187: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:187: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:198: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  pd.period_range(\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/dataset/multivariate_grouper.py:198: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  pd.period_range(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name  | Type          | Params | In sizes                                                         | Out sizes      \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | TimeGradModel | 73.4 K | [[1, 1], [1, 1], [1, 90, 4], [1, 90, 8], [1, 90, 8], [1, 30, 4]] | [1, 100, 30, 8]\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "73.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "73.4 K    Total params\n",
      "0.294     Total estimated model params size (MB)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/feature.py:364: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  index = pd.period_range(start, periods=length, freq=start.freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/feature.py:364: FutureWarning: PeriodDtype[B] is deprecated and will be removed in a future version. Use a DatetimeIndex with freq='B' instead\n",
      "  index = pd.period_range(start, periods=length, freq=start.freq)\n",
      "/opt/homebrew/anaconda3/envs/ysda_env/lib/python3.11/site-packages/gluonts/transform/split.py:150: FutureWarning: Period with BDay freq is deprecated and will be removed in a future version. Use a DatetimeIndex with BDay freq instead.\n",
      "  entry[self.start_field] + idx + self.lead_time\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1092ffb7814a5683adca0f8f9211c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 64: 'train_loss' reached 0.41521 (best 0.41521), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=0-step=64.ckpt' as top 1\n",
      "Epoch 1, global step 128: 'train_loss' reached 0.28120 (best 0.28120), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=1-step=128.ckpt' as top 1\n",
      "Epoch 2, global step 192: 'train_loss' reached 0.08373 (best 0.08373), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=2-step=192.ckpt' as top 1\n",
      "Epoch 3, global step 256: 'train_loss' reached 0.04662 (best 0.04662), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=3-step=256.ckpt' as top 1\n",
      "Epoch 4, global step 320: 'train_loss' reached 0.03065 (best 0.03065), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=4-step=320.ckpt' as top 1\n",
      "Epoch 5, global step 384: 'train_loss' reached 0.02148 (best 0.02148), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=5-step=384.ckpt' as top 1\n",
      "Epoch 6, global step 448: 'train_loss' reached 0.01611 (best 0.01611), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=6-step=448.ckpt' as top 1\n",
      "Epoch 7, global step 512: 'train_loss' reached 0.01328 (best 0.01328), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=7-step=512.ckpt' as top 1\n",
      "Epoch 8, global step 576: 'train_loss' reached 0.01189 (best 0.01189), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=8-step=576.ckpt' as top 1\n",
      "Epoch 9, global step 640: 'train_loss' reached 0.01063 (best 0.01063), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=9-step=640.ckpt' as top 1\n",
      "Epoch 10, global step 704: 'train_loss' reached 0.01004 (best 0.01004), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=10-step=704.ckpt' as top 1\n",
      "Epoch 11, global step 768: 'train_loss' reached 0.00924 (best 0.00924), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=11-step=768.ckpt' as top 1\n",
      "Epoch 12, global step 832: 'train_loss' reached 0.00840 (best 0.00840), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=12-step=832.ckpt' as top 1\n",
      "Epoch 13, global step 896: 'train_loss' reached 0.00801 (best 0.00801), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=13-step=896.ckpt' as top 1\n",
      "Epoch 14, global step 960: 'train_loss' was not in top 1\n",
      "Epoch 15, global step 1024: 'train_loss' reached 0.00759 (best 0.00759), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=15-step=1024.ckpt' as top 1\n",
      "Epoch 16, global step 1088: 'train_loss' was not in top 1\n",
      "Epoch 17, global step 1152: 'train_loss' was not in top 1\n",
      "Epoch 18, global step 1216: 'train_loss' reached 0.00706 (best 0.00706), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=18-step=1216.ckpt' as top 1\n",
      "Epoch 19, global step 1280: 'train_loss' was not in top 1\n",
      "Epoch 20, global step 1344: 'train_loss' was not in top 1\n",
      "Epoch 21, global step 1408: 'train_loss' reached 0.00657 (best 0.00657), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=21-step=1408.ckpt' as top 1\n",
      "Epoch 22, global step 1472: 'train_loss' reached 0.00642 (best 0.00642), saving model to '/Users/npbukhanchenko/Desktop/THESIS/lightning_logs/version_3/checkpoints/epoch=22-step=1472.ckpt' as top 1\n",
      "Epoch 23, global step 1536: 'train_loss' was not in top 1\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"solar_nips\": [],\n",
    "    \"electricity_nips\": [],\n",
    "    \"exchange_rate_nips\": []\n",
    "}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    dataset = prepare_dataset(dataset_name)\n",
    "    predictor = prepare_predictor(dataset)\n",
    "    forecasts, targets, agg_metric = prepare_metrics(dataset, predictor)\n",
    "    datasets[dataset_name] = {\n",
    "        \"dataset\": dataset,\n",
    "        \"predictor\": predictor,\n",
    "        \"forecasts\": forecasts,\n",
    "        \"targets\": targets,\n",
    "        \"agg_metric\": agg_metric\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"solar_nips\"][\"dataset\"], datasets[\"solar_nips\"][\"forecasts\"],\n",
    "    datasets[\"solar_nips\"][\"targets\"], datasets[\"solar_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"electricity_nips\"][\"dataset\"], datasets[\"electricity_nips\"][\"forecasts\"],\n",
    "    datasets[\"electricity_nips\"][\"targets\"], datasets[\"electricity_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_statistics(\n",
    "    datasets[\"exchange_rate_nips\"][\"dataset\"], datasets[\"exchange_rate_nips\"][\"forecasts\"],\n",
    "    datasets[\"exchange_rate_nips\"][\"targets\"], datasets[\"exchange_rate_nips\"][\"agg_metric\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
